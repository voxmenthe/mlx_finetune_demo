{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AWQ (Activation-aware Weight Quantization) with MLX-LM\n",
    "\n",
    "This notebook demonstrates how to use AWQ (Activation-aware Weight Quantization) with MLX-LM to scale and clip weights before quantization.\n",
    "\n",
    "## What is AWQ?\n",
    "AWQ is a quantization method that scales and clips weights before quantization to preserve model quality. It uses calibration samples to determine optimal scaling factors for different weights.\n",
    "\n",
    "## Requirements\n",
    "- macOS with Apple Silicon (M1/M2/M3/M4)\n",
    "- Python 3.9+\n",
    "- MLX framework\n",
    "- Sufficient disk space for model storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Environment Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "# Environment setup\n",
    "print(\"Setting up environment for AWQ quantization...\")\n",
    "\n",
    "# Create project directories\n",
    "project_dir = Path.cwd()\n",
    "models_dir = project_dir / \"models\"\n",
    "models_dir.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"Project directory: {project_dir}\")\n",
    "print(f\"Models directory: {models_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Install MLX and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "print(\"Installing MLX and dependencies...\")\n",
    "\n",
    "packages = [\n",
    "    \"mlx-lm\",\n",
    "    \"transformers\",\n",
    "    \"torch\", \n",
    "    \"huggingface_hub\",\n",
    "    \"datasets\",\n",
    "    \"accelerate\",\n",
    "    \"sentencepiece\",\n",
    "    \"protobuf\"\n",
    "]\n",
    "\n",
    "for package in packages:\n",
    "    try:\n",
    "        print(f\"Installing {package}...\")\n",
    "        subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", package], \n",
    "                      check=True, capture_output=True, text=True)\n",
    "        print(f\"✅ {package} installed successfully\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"⚠️ Warning installing {package}: {e}\")\n",
    "\n",
    "print(\"\\n📦 All packages installation completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Test MLX Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test imports\n",
    "print(\"Testing MLX imports...\")\n",
    "\n",
    "try:\n",
    "    import mlx.core as mx\n",
    "    from mlx_lm import load, generate\n",
    "    from huggingface_hub import login, snapshot_download\n",
    "    print(\"✅ All imports successful!\")\n",
    "    \n",
    "    # Test MLX functionality\n",
    "    test_array = mx.array([1, 2, 3])\n",
    "    print(f\"✅ MLX test array: {test_array}\")\n",
    "    \n",
    "except ImportError as e:\n",
    "    print(f\"❌ Import failed: {e}\")\n",
    "    print(\"Please restart kernel and try again.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AWQ Configuration\n",
    "print(\"=== AWQ Configuration ===\\n\")\n",
    "\n",
    "# Model to quantize (you can change this)\n",
    "MODEL_NAME = \"Qwen/Qwen2.5-0.5B\"  # Small model for demonstration\n",
    "\n",
    "# AWQ Parameters\n",
    "AWQ_CONFIG = {\n",
    "    \"bits\": 4,                    # Quantization precision (typically 4 bits)\n",
    "    \"num_samples\": 32,            # Calibration samples (default: 32)\n",
    "    \"n_grid\": 10,                 # Search granularity (default: 10)\n",
    "    \"group_size\": 128,            # Group size for quantization\n",
    "}\n",
    "\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(f\"Target bits: {AWQ_CONFIG['bits']}\")\n",
    "print(f\"Calibration samples: {AWQ_CONFIG['num_samples']}\")\n",
    "print(f\"Search grid: {AWQ_CONFIG['n_grid']}\")\n",
    "print(f\"Group size: {AWQ_CONFIG['group_size']}\")\n",
    "\n",
    "# Set up directories\n",
    "original_model_dir = models_dir / MODEL_NAME.replace(\"/\", \"_\")\n",
    "awq_model_dir = models_dir / f\"{MODEL_NAME.replace('/', '_')}_AWQ_{AWQ_CONFIG['bits']}bit\"\n",
    "\n",
    "print(f\"\\nOriginal model dir: {original_model_dir}\")\n",
    "print(f\"AWQ model dir: {awq_model_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Download Original Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "print(f\"Downloading {MODEL_NAME}...\")\n",
    "print(\"This may take a while depending on model size and internet connection.\")\n",
    "\n",
    "# Create directories\n",
    "original_model_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Check if model already exists\n",
    "if list(original_model_dir.glob(\"*\")):\n",
    "    print(f\"Model files found in {original_model_dir}\")\n",
    "    use_existing = input(\"Use existing model files? (y/n): \").strip().lower()\n",
    "    if use_existing != 'y':\n",
    "        import shutil\n",
    "        shutil.rmtree(original_model_dir)\n",
    "        original_model_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "if not list(original_model_dir.glob(\"*\")):\n",
    "    try:\n",
    "        start_time = datetime.now()\n",
    "        \n",
    "        downloaded_path = snapshot_download(\n",
    "            repo_id=MODEL_NAME,\n",
    "            local_dir=str(original_model_dir),\n",
    "            local_dir_use_symlinks=False\n",
    "        )\n",
    "        \n",
    "        end_time = datetime.now()\n",
    "        duration = end_time - start_time\n",
    "        \n",
    "        print(f\"✅ Model downloaded successfully in {duration}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Download failed: {e}\")\n",
    "        print(\"Please check the model name and internet connection.\")\n",
    "\n",
    "# List downloaded files\n",
    "print(\"\\nModel files:\")\n",
    "total_size = 0\n",
    "for file in original_model_dir.glob(\"*\"):\n",
    "    if file.is_file():\n",
    "        size_mb = file.stat().st_size / 1024 / 1024\n",
    "        total_size += size_mb\n",
    "        print(f\"  {file.name} ({size_mb:.2f} MB)\")\n",
    "\n",
    "print(f\"\\nTotal model size: {total_size:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: AWQ Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"Starting AWQ quantization...\")\n",
    "print(f\"Source: {original_model_dir}\")\n",
    "print(f\"Target: {awq_model_dir}\")\n",
    "print(f\"Configuration: {AWQ_CONFIG}\")\n",
    "\n",
    "# Clean up existing AWQ directory\n",
    "if awq_model_dir.exists():\n",
    "    print(f\"Removing existing AWQ directory: {awq_model_dir}\")\n",
    "    shutil.rmtree(awq_model_dir)\n",
    "\n",
    "awq_model_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Build AWQ command\n",
    "awq_cmd = [\n",
    "    \"python\", \"-m\", \"mlx_lm.awq\",\n",
    "    \"--model\", str(original_model_dir),\n",
    "    \"--mlx-path\", str(awq_model_dir),\n",
    "    \"--bits\", str(AWQ_CONFIG[\"bits\"]),\n",
    "    \"--num-samples\", str(AWQ_CONFIG[\"num_samples\"]),\n",
    "    \"--n-grid\", str(AWQ_CONFIG[\"n_grid\"])\n",
    "]\n",
    "\n",
    "print(f\"\\nRunning command: {' '.join(awq_cmd)}\")\n",
    "\n",
    "try:\n",
    "    start_time = datetime.now()\n",
    "    \n",
    "    # Run AWQ quantization\n",
    "    result = subprocess.run(\n",
    "        awq_cmd,\n",
    "        capture_output=True,\n",
    "        text=True,\n",
    "        cwd=str(project_dir)\n",
    "    )\n",
    "    \n",
    "    end_time = datetime.now()\n",
    "    duration = end_time - start_time\n",
    "    \n",
    "    if result.returncode == 0:\n",
    "        print(f\"\\n✅ AWQ quantization completed successfully in {duration}!\")\n",
    "        print(\"STDOUT:\", result.stdout)\n",
    "    else:\n",
    "        print(f\"\\n❌ AWQ quantization failed!\")\n",
    "        print(\"STDERR:\", result.stderr)\n",
    "        print(\"STDOUT:\", result.stdout)\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error running AWQ: {e}\")\n",
    "\n",
    "# Check results\n",
    "if awq_model_dir.exists() and list(awq_model_dir.glob(\"*\")):\n",
    "    print(\"\\nAWQ quantized files:\")\n",
    "    total_awq_size = 0\n",
    "    for file in awq_model_dir.glob(\"*\"):\n",
    "        if file.is_file():\n",
    "            size_mb = file.stat().st_size / 1024 / 1024\n",
    "            total_awq_size += size_mb\n",
    "            print(f\"  {file.name} ({size_mb:.2f} MB)\")\n",
    "    \n",
    "    print(f\"\\nTotal AWQ model size: {total_awq_size:.2f} MB\")\n",
    "    if total_size > 0:\n",
    "        print(f\"Size reduction: {((total_size - total_awq_size) / total_size * 100):.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Test AWQ Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the AWQ quantized model\n",
    "if awq_model_dir.exists() and list(awq_model_dir.glob(\"*\")):\n",
    "    print(\"Testing AWQ quantized model...\")\n",
    "    \n",
    "    try:\n",
    "        # Load the AWQ model\n",
    "        model, tokenizer = load(str(awq_model_dir))\n",
    "        print(\"✅ AWQ model loaded successfully!\")\n",
    "        \n",
    "        # Test generation\n",
    "        test_prompts = [\n",
    "            \"Hello, how are you?\",\n",
    "            \"The weather today is\",\n",
    "            \"Artificial intelligence is\",\n",
    "            \"Machine learning can be used for\"\n",
    "        ]\n",
    "        \n",
    "        print(\"\\n=== AWQ Model Test Results ===\")\n",
    "        for prompt in test_prompts:\n",
    "            print(f\"\\nPrompt: '{prompt}'\")\n",
    "            \n",
    "            response = generate(\n",
    "                model, \n",
    "                tokenizer, \n",
    "                prompt=prompt, \n",
    "                max_tokens=50,\n",
    "                temp=0.7\n",
    "            )\n",
    "            \n",
    "            print(f\"Response: {response}\")\n",
    "            \n",
    "        print(\"\\n✅ AWQ model is working correctly!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error testing AWQ model: {e}\")\nelse:\n",
    "    print(\"❌ AWQ model not found. Quantization may have failed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Compare Original vs AWQ Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Compare original vs AWQ model performance\n",
    "import time\n",
    "\n",
    "compare_models = input(\"Do you want to compare original vs AWQ model performance? (y/n): \").strip().lower()\n",
    "\n",
    "if compare_models == 'y':\n",
    "    print(\"\\n=== Model Performance Comparison ===\")\n",
    "    \n",
    "    test_prompt = \"The future of artificial intelligence\"\n",
    "    max_tokens = 100\n",
    "    \n",
    "    try:\n",
    "        # Test original model\n",
    "        print(\"\\n🔄 Testing original model...\")\n",
    "        original_model, original_tokenizer = load(str(original_model_dir))\n",
    "        \n",
    "        start_time = time.time()\n",
    "        original_response = generate(\n",
    "            original_model, \n",
    "            original_tokenizer, \n",
    "            prompt=test_prompt, \n",
    "            max_tokens=max_tokens,\n",
    "            temp=0.7\n",
    "        )\n",
    "        original_time = time.time() - start_time\n",
    "        \n",
    "        print(f\"Original response: {original_response}\")\n",
    "        print(f\"Original generation time: {original_time:.2f}s\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error testing original model: {e}\")\n",
    "        original_response = None\n",
    "        original_time = None\n",
    "    \n",
    "    try:\n",
    "        # Test AWQ model (already loaded above)\n",
    "        print(\"\\n🔄 Testing AWQ model...\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "        awq_response = generate(\n",
    "            model, \n",
    "            tokenizer, \n",
    "            prompt=test_prompt, \n",
    "            max_tokens=max_tokens,\n",
    "            temp=0.7\n",
    "        )\n",
    "        awq_time = time.time() - start_time\n",
    "        \n",
    "        print(f\"AWQ response: {awq_response}\")\n",
    "        print(f\"AWQ generation time: {awq_time:.2f}s\")\n",
    "        \n",
    "        # Compare performance\n",
    "        if original_time and awq_time:\n",
    "            speedup = original_time / awq_time\n",
    "            print(f\"\\n📊 Performance comparison:\")\n",
    "            print(f\"   Speedup: {speedup:.2f}x\")\n",
    "            print(f\"   Time saved: {original_time - awq_time:.2f}s\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error testing AWQ model: {e}\")\nelse:\n",
    "    print(\"Skipping performance comparison.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Evaluate Model Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Evaluate the quantized model\n",
    "print(\"=== Model Quality Evaluation ===\\n\")\n",
    "\n",
    "evaluate_model = input(\"Do you want to evaluate model quality? (y/n): \").strip().lower()\n",
    "\n",
    "if evaluate_model == 'y':\n",
    "    # You can use mlx_lm.evaluate for this\n",
    "    eval_cmd = [\n",
    "        \"python\", \"-m\", \"mlx_lm.evaluate\",\n",
    "        \"--model\", str(awq_model_dir),\n",
    "        \"--dataset\", \"wikitext\",  # or your preferred dataset\n",
    "        \"--few-shot\", \"5\"\n",
    "    ]\n",
    "    \n",
    "    print(f\"Running evaluation: {' '.join(eval_cmd)}\")\n",
    "    \n",
    "    try:\n",
    "        result = subprocess.run(eval_cmd, capture_output=True, text=True)\n",
    "        \n",
    "        if result.returncode == 0:\n",
    "            print(\"\\n✅ Evaluation completed!\")\n",
    "            print(result.stdout)\n",
    "        else:\n",
    "            print(\"\\n❌ Evaluation failed!\")\n",
    "            print(result.stderr)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error running evaluation: {e}\")\nelse:\n",
    "    print(\"Skipping evaluation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Upload to Hugging Face (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi, upload_folder\n",
    "import getpass\n",
    "\n",
    "upload_to_hf = input(\"Do you want to upload the AWQ model to Hugging Face? (y/n): \").strip().lower()\n",
    "\n",
    "if upload_to_hf == 'y':\n",
    "    # Get Hugging Face credentials\n",
    "    print(\"Please enter your Hugging Face token:\")\n",
    "    hf_token = getpass.getpass(\"HF Token: \")\n",
    "    \n",
    "    try:\n",
    "        login(token=hf_token)\n",
    "        print(\"✅ Successfully logged in to Hugging Face!\")\n",
    "        \n",
    "        # Get repository name\n",
    "        repo_name = input(\"Enter repository name (e.g., 'username/model-name-awq'): \").strip()\n",
    "        \n",
    "        # Create repository\n",
    "        api = HfApi()\n",
    "        api.create_repo(repo_id=repo_name, repo_type=\"model\", exist_ok=True)\n",
    "        print(f\"✅ Repository {repo_name} created!\")\n",
    "        \n",
    "        # Create model card\n",
    "        model_card = f\"\"\"---\n",
    "license: apache-2.0\n",
    "base_model: {MODEL_NAME}\n",
    "tags:\n",
    "- mlx\n",
    "- awq\n",
    "- quantized\n",
    "- {AWQ_CONFIG['bits']}-bit\n",
    "---\n",
    "\n",
    "# {MODEL_NAME.split('/')[-1]} - AWQ {AWQ_CONFIG['bits']}-bit\n",
    "\n",
    "This is an AWQ (Activation-aware Weight Quantization) {AWQ_CONFIG['bits']}-bit quantized version of [{MODEL_NAME}](https://huggingface.co/{MODEL_NAME}).\n",
    "\n",
    "## Quantization Details\n",
    "- Method: AWQ (Activation-aware Weight Quantization)\n",
    "- Precision: {AWQ_CONFIG['bits']}-bit\n",
    "- Calibration samples: {AWQ_CONFIG['num_samples']}\n",
    "- Search grid: {AWQ_CONFIG['n_grid']}\n",
    "- Group size: {AWQ_CONFIG['group_size']}\n",
    "\n",
    "## Features\n",
    "- Scales and clips weights before quantization\n",
    "- Optimized for Apple Silicon devices\n",
    "- Maintains model quality through activation-aware scaling\n",
    "\n",
    "## Usage\n",
    "```python\n",
    "from mlx_lm import load, generate\n",
    "\n",
    "model, tokenizer = load(\"{repo_name}\")\n",
    "response = generate(model, tokenizer, prompt=\"Hello\", max_tokens=100)\n",
    "```\n",
    "\"\"\"\n",
    "        \n",
    "        # Save model card\n",
    "        with open(awq_model_dir / \"README.md\", \"w\") as f:\n",
    "            f.write(model_card)\n",
    "        \n",
    "        # Upload\n",
    "        print(f\"Uploading to {repo_name}...\")\n",
    "        upload_folder(\n",
    "            folder_path=str(awq_model_dir),\n",
    "            repo_id=repo_name,\n",
    "            repo_type=\"model\",\n",
    "            commit_message=f\"Add AWQ {AWQ_CONFIG['bits']}-bit quantized model\"\n",
    "        )\n",
    "        \n",
    "        print(f\"✅ Model uploaded successfully!\")\n",
    "        print(f\"🔗 https://huggingface.co/{repo_name}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Upload failed: {e}\")\nelse:\n",
    "    print(\"Skipping upload.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11: Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"🎉 AWQ QUANTIZATION SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\n📋 Configuration:\")\n",
    "print(f\"   Base Model: {MODEL_NAME}\")\n",
    "print(f\"   Target Bits: {AWQ_CONFIG['bits']}\")\n",
    "print(f\"   Calibration Samples: {AWQ_CONFIG['num_samples']}\")\n",
    "print(f\"   Search Grid: {AWQ_CONFIG['n_grid']}\")\n",
    "print(f\"   Group Size: {AWQ_CONFIG['group_size']}\")\n",
    "\n",
    "print(f\"\\n📁 Directories:\")\n",
    "print(f\"   Original: {original_model_dir}\")\n",
    "print(f\"   AWQ Model: {awq_model_dir}\")\n",
    "\n",
    "# Check if quantization was successful\n",
    "if awq_model_dir.exists() and list(awq_model_dir.glob(\"*\")):\n",
    "    print(f\"\\n✅ Status: AWQ quantization completed successfully!\")\n",
    "    \n",
    "    # Calculate size reduction if possible\n",
    "    if 'total_size' in locals() and 'total_awq_size' in locals():\n",
    "        print(f\"   Original size: {total_size:.2f} MB\")\n",
    "        print(f\"   AWQ size: {total_awq_size:.2f} MB\")\n",
    "        print(f\"   Size reduction: {((total_size - total_awq_size) / total_size * 100):.1f}%\")\nelse:\n",
    "    print(f\"\\n❌ Status: AWQ quantization failed or incomplete\")\n",
    "\n",
    "print(f\"\\n💡 AWQ Advantages:\")\n",
    "print(f\"   • Scales and clips weights before quantization\")\n",
    "print(f\"   • Preserves model quality through activation awareness\")\n",
    "print(f\"   • Efficient search for optimal scaling factors\")\n",
    "print(f\"   • Good balance between size and performance\")\n",
    "\n",
    "print(f\"\\n🔧 Tuning Tips:\")\n",
    "print(f\"   • Increase num_samples for better quality\")\n",
    "print(f\"   • Increase n_grid for more thorough search\")\n",
    "print(f\"   • Adjust group_size based on model architecture\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Thank you for using AWQ quantization!\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}