{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Universal MLX Model Converter\n",
    "\n",
    "This notebook converts any Hugging Face model to MLX format and uploads it to Hugging Face.\n",
    "\n",
    "## Features\n",
    "- Convert any compatible model to MLX format\n",
    "- Automatic model download and organization\n",
    "- Configurable quantization options\n",
    "- Automatic upload to Hugging Face\n",
    "\n",
    "## Requirements\n",
    "- macOS with Apple Silicon (M1/M2/M3/M4)\n",
    "- Python 3.9+\n",
    "- Sufficient disk space (varies by model size)\n",
    "- Hugging Face account and token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Configuration - Enter Your Model Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Configuration\n",
    "print(\"=== Universal MLX Model Converter ===\")\n",
    "print(\"Please enter the details for the model you want to convert:\\n\")\n",
    "\n",
    "# Get user inputs\n",
    "SOURCE_MODEL = input(\"Enter the source model name (e.g., 'microsoft/DialoGPT-medium'): \").strip()\n",
    "TARGET_REPO = input(\"Enter your target repository name (e.g., 'username/model-name-mlx'): \").strip()\n",
    "HF_USERNAME = input(\"Enter your Hugging Face username: \").strip()\n",
    "\n",
    "# Quantization options\n",
    "print(\"\\n=== Quantization Options ===\")\n",
    "print(\"1. No quantization (largest size, best quality)\")\n",
    "print(\"2. 4-bit quantization (recommended, good balance)\")\n",
    "print(\"3. 8-bit quantization (larger than 4-bit, better quality)\")\n",
    "quant_choice = input(\"Choose quantization option (1-3): \").strip()\n",
    "\n",
    "# Set quantization parameters\n",
    "if quant_choice == \"2\":\n",
    "    USE_QUANTIZATION = True\n",
    "    Q_BITS = 4\n",
    "    Q_GROUP_SIZE = 64\n",
    "elif quant_choice == \"3\":\n",
    "    USE_QUANTIZATION = True\n",
    "    Q_BITS = 8\n",
    "    Q_GROUP_SIZE = 128\n",
    "else:\n",
    "    USE_QUANTIZATION = False\n",
    "    Q_BITS = None\n",
    "    Q_GROUP_SIZE = None\n",
    "\n",
    "print(f\"\\n=== Configuration Summary ===\")\n",
    "print(f\"Source Model: {SOURCE_MODEL}\")\n",
    "print(f\"Target Repository: {TARGET_REPO}\")\n",
    "print(f\"Username: {HF_USERNAME}\")\n",
    "print(f\"Quantization: {'Yes' if USE_QUANTIZATION else 'No'}\")\n",
    "if USE_QUANTIZATION:\n",
    "    print(f\"  - Bits: {Q_BITS}\")\n",
    "    print(f\"  - Group Size: {Q_GROUP_SIZE}\")\n",
    "\n",
    "confirm = input(\"\\nProceed with these settings? (y/n): \").strip().lower()\n",
    "if confirm != 'y':\n",
    "    print(\"Configuration cancelled. Please restart and enter new values.\")\n",
    "    import sys\n",
    "    sys.exit(1)\n",
    "else:\n",
    "    print(\"‚úÖ Configuration confirmed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "# Create safe directory names from model names\n",
    "def create_safe_dirname(name):\n",
    "    \"\"\"Convert model name to safe directory name\"\"\"\n",
    "    return re.sub(r'[^a-zA-Z0-9_-]', '_', name.replace('/', '_'))\n",
    "\n",
    "# Set up project directories\n",
    "project_dir = Path.cwd()\n",
    "models_dir = project_dir / \"models\"\n",
    "source_model_dirname = create_safe_dirname(SOURCE_MODEL)\n",
    "target_model_dirname = create_safe_dirname(TARGET_REPO.split('/')[-1])  # Use just the model name part\n",
    "\n",
    "original_model_dir = models_dir / source_model_dirname\n",
    "mlx_model_dir = models_dir / f\"{target_model_dirname}_mlx\"\n",
    "\n",
    "# Create directories\n",
    "models_dir.mkdir(exist_ok=True)\n",
    "original_model_dir.mkdir(exist_ok=True)\n",
    "mlx_model_dir.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"Project directory: {project_dir}\")\n",
    "print(f\"Models directory: {models_dir}\")\n",
    "print(f\"Source model directory: {original_model_dir}\")\n",
    "print(f\"MLX model directory: {mlx_model_dir}\")\n",
    "\n",
    "# Environment setup\n",
    "print(\"\\nSetting up environment...\")\n",
    "try:\n",
    "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"--upgrade\", \"pip\"], check=True, capture_output=True)\n",
    "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"--upgrade\", \"numpy\"], check=True, capture_output=True)\n",
    "    print(\"‚úÖ Core packages updated successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Package update warning: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "print(\"Installing MLX and dependencies...\")\n",
    "\n",
    "packages = [\n",
    "    \"mlx-lm\",\n",
    "    \"transformers\",\n",
    "    \"torch\", \n",
    "    \"huggingface_hub\",\n",
    "    \"datasets\",\n",
    "    \"accelerate\",\n",
    "    \"sentencepiece\",\n",
    "    \"protobuf\"\n",
    "]\n",
    "\n",
    "failed_packages = []\n",
    "for package in packages:\n",
    "    try:\n",
    "        print(f\"Installing {package}...\")\n",
    "        result = subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", package], \n",
    "                              check=True, capture_output=True, text=True)\n",
    "        print(f\"‚úÖ {package} installed successfully\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"‚ùå Failed to install {package}: {e}\")\n",
    "        failed_packages.append(package)\n",
    "\n",
    "if failed_packages:\n",
    "    print(f\"\\n‚ö†Ô∏è Failed to install: {', '.join(failed_packages)}\")\n",
    "    print(\"You may need to install these manually or restart the kernel.\")\nelse:\n",
    "    print(\"\\nüéâ All packages installed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Test Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test imports\n",
    "print(\"Testing imports...\")\n",
    "import_success = True\n",
    "\n",
    "try:\n",
    "    import numpy as np\n",
    "    print(\"‚úÖ numpy imported successfully\")\nexcept ImportError as e:\n",
    "    print(f\"‚ùå numpy import failed: {e}\")\n",
    "    import_success = False\n",
    "\n",
    "if import_success:\n",
    "    try:\n",
    "        import mlx.core as mx\n",
    "        print(\"‚úÖ mlx.core imported successfully\")\n",
    "    except ImportError as e:\n",
    "        print(f\"‚ùå mlx.core import failed: {e}\")\n",
    "        import_success = False\n",
    "\n",
    "    try:\n",
    "        from mlx_lm import convert, load, generate\n",
    "        print(\"‚úÖ mlx_lm imported successfully\")\n",
    "    except ImportError as e:\n",
    "        print(f\"‚ùå mlx_lm import failed: {e}\")\n",
    "        import_success = False\n",
    "\n",
    "    try:\n",
    "        from huggingface_hub import login, HfApi, snapshot_download, upload_folder\n",
    "        print(\"‚úÖ huggingface_hub imported successfully\")\n",
    "    except ImportError as e:\n",
    "        print(f\"‚ùå huggingface_hub import failed: {e}\")\n",
    "        import_success = False\n",
    "\n",
    "if import_success:\n",
    "    print(\"\\nüéâ All imports successful! Ready to proceed.\")\nelse:\n",
    "    print(\"\\n‚ö†Ô∏è Some imports failed. Please restart kernel and try again.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Hugging Face Authentication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "\n",
    "# Login to Hugging Face\n",
    "print(\"Please enter your Hugging Face token:\")\n",
    "print(\"(You can get it from: https://huggingface.co/settings/tokens)\")\n",
    "hf_token = getpass.getpass(\"HF Token: \")\n",
    "\n",
    "try:\n",
    "    login(token=hf_token)\n",
    "    print(\"‚úÖ Successfully logged in to Hugging Face!\")\n",
    "    \n",
    "    # Initialize HF API\n",
    "    api = HfApi()\n",
    "    \n",
    "    # Test API access\n",
    "    user_info = api.whoami()\n",
    "    print(f\"Logged in as: {user_info['name']}\")\n",
    "    \nexcept Exception as e:\n",
    "    print(f\"‚ùå Failed to login: {e}\")\n",
    "    print(\"Please check your token and try again.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Download Source Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "from datetime import datetime\n",
    "\n",
    "print(f\"Downloading {SOURCE_MODEL}...\")\n",
    "print(f\"This may take a while depending on model size and internet connection.\")\n",
    "print(f\"Download location: {original_model_dir}\")\n",
    "\n",
    "# Check if model already exists\n",
    "if list(original_model_dir.glob(\"*\")):\n",
    "    print(f\"Model files found in {original_model_dir}\")\n",
    "    redownload = input(\"Re-download the model? (y/n): \").strip().lower()\n",
    "    if redownload == 'y':\n",
    "        shutil.rmtree(original_model_dir)\n",
    "        original_model_dir.mkdir(exist_ok=True)\n",
    "    else:\n",
    "        print(\"Using existing model files.\")\n",
    "\n",
    "if not list(original_model_dir.glob(\"*\")):\n",
    "    try:\n",
    "        start_time = datetime.now()\n",
    "        \n",
    "        # Download the model\n",
    "        downloaded_path = snapshot_download(\n",
    "            repo_id=SOURCE_MODEL,\n",
    "            local_dir=str(original_model_dir),\n",
    "            local_dir_use_symlinks=False,\n",
    "            resume_download=True\n",
    "        )\n",
    "        \n",
    "        end_time = datetime.now()\n",
    "        duration = end_time - start_time\n",
    "        \n",
    "        print(f\"‚úÖ Model downloaded successfully in {duration}\")\n",
    "        print(f\"Download location: {downloaded_path}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Download failed: {e}\")\n",
    "        print(\"Please check the model name and your internet connection.\")\n",
    "\n",
    "# List downloaded files\n",
    "print(\"\\nDownloaded files:\")\n",
    "total_size = 0\n",
    "for file in original_model_dir.glob(\"*\"):\n",
    "    if file.is_file():\n",
    "        size_mb = file.stat().st_size / 1024 / 1024\n",
    "        total_size += size_mb\n",
    "        print(f\"  {file.name} ({size_mb:.2f} MB)\")\n",
    "\nprint(f\"\\nTotal model size: {total_size:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Convert to MLX Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"Starting MLX conversion...\")\n",
    "print(f\"Source: {original_model_dir}\")\n",
    "print(f\"Destination: {mlx_model_dir}\")\n",
    "print(f\"Quantization: {'Enabled' if USE_QUANTIZATION else 'Disabled'}\")\n",
    "\n",
    "# Clean up existing MLX directory\n",
    "if mlx_model_dir.exists() and list(mlx_model_dir.glob(\"*\")):\n",
    "    print(f\"Cleaning existing MLX directory: {mlx_model_dir}\")\n",
    "    shutil.rmtree(mlx_model_dir)\n",
    "    mlx_model_dir.mkdir(exist_ok=True)\n",
    "\n",
    "conversion_success = False\n",
    "start_time = datetime.now()\n",
    "\n",
    "# Try different conversion methods\n",
    "methods = [\n",
    "    {\"name\": \"Standard with quantization\", \"func\": lambda: convert(\n",
    "        str(original_model_dir), \n",
    "        str(mlx_model_dir),\n",
    "        quantize=USE_QUANTIZATION,\n",
    "        q_group_size=Q_GROUP_SIZE,\n",
    "        q_bits=Q_BITS\n",
    "    ) if USE_QUANTIZATION else None},\n",
    "    {\"name\": \"Standard without quantization\", \"func\": lambda: convert(\n",
    "        str(original_model_dir), \n",
    "        str(mlx_model_dir)\n",
    "    )},\n",
    "    {\"name\": \"Command line conversion\", \"func\": lambda: subprocess.run([\n",
    "        sys.executable, \"-m\", \"mlx_lm.convert\",\n",
    "        \"--hf-path\", str(original_model_dir),\n",
    "        \"--mlx-path\", str(mlx_model_dir)\n",
    "    ] + ([\"--quantize\"] if USE_QUANTIZATION else []), check=True)}\n",
    "]\n",
    "\n",
    "for method in methods:\n",
    "    if method[\"func\"] is None:  # Skip if quantization method not applicable\n",
    "        continue\n",
    "        \n",
    "    try:\n",
    "        print(f\"\\nTrying: {method['name']}...\")\n",
    "        method[\"func\"]()\n",
    "        print(f\"‚úÖ {method['name']} completed successfully!\")\n",
    "        conversion_success = True\n",
    "        break\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå {method['name']} failed: {e}\")\n",
    "        continue\n",
    "\n",
    "end_time = datetime.now()\n",
    "duration = end_time - start_time\n",
    "\n",
    "if conversion_success:\n",
    "    print(f\"\\nüéâ MLX conversion completed successfully in {duration}!\")\n",
    "    \n",
    "    # Verify conversion results\n",
    "    print(\"\\nConverted MLX files:\")\n",
    "    total_size = 0\n",
    "    for file in mlx_model_dir.glob(\"*\"):\n",
    "        if file.is_file():\n",
    "            size_mb = file.stat().st_size / 1024 / 1024\n",
    "            total_size += size_mb\n",
    "            print(f\"  {file.name} ({size_mb:.2f} MB)\")\n",
    "    \n",
    "    print(f\"\\nTotal MLX model size: {total_size:.2f} MB\")\nelse:\n",
    "    print(f\"\\n‚ùå All conversion methods failed. Please check the model compatibility.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Test MLX Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if conversion_success:\n",
    "    print(\"Testing the converted MLX model...\")\n",
    "    \n",
    "    try:\n",
    "        # Load the converted MLX model\n",
    "        model, tokenizer = load(str(mlx_model_dir))\n",
    "        print(\"‚úÖ MLX model loaded successfully!\")\n",
    "        \n",
    "        # Test generation with a simple prompt\n",
    "        test_prompt = \"Hello, this is a test\"\n",
    "        print(f\"\\nTesting with prompt: '{test_prompt}'\")\n",
    "        \n",
    "        response = generate(\n",
    "            model, \n",
    "            tokenizer, \n",
    "            prompt=test_prompt, \n",
    "            max_tokens=50,\n",
    "            temp=0.7\n",
    "        )\n",
    "        \n",
    "        print(f\"Generated response: {response}\")\n",
    "        print(\"\\n‚úÖ MLX model is working correctly!\")\n",
    "        \n",
    "        model_test_success = True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error testing MLX model: {e}\")\n",
    "        print(\"The model converted but may have compatibility issues.\")\n",
    "        model_test_success = False\nelse:\n",
    "    print(\"Skipping model test due to conversion failure.\")\n",
    "    model_test_success = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Create Model Documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if conversion_success:\n",
    "    from datetime import datetime\n",
    "    \n",
    "    # Create model card\n",
    "    model_card_content = f\"\"\"---\n",
    "license: apache-2.0\n",
    "base_model: {SOURCE_MODEL}\n",
    "tags:\n",
    "- mlx\n",
    "- converted\n",
    "- apple-silicon\n",
    "{'- quantized' if USE_QUANTIZATION else ''}\n",
    "---\n",
    "\n",
    "# {TARGET_REPO.split('/')[-1]}\n",
    "\n",
    "This is an MLX-optimized version of [{SOURCE_MODEL}](https://huggingface.co/{SOURCE_MODEL}), converted for Apple Silicon devices.\n",
    "\n",
    "## Model Details\n",
    "\n",
    "- **Base Model**: {SOURCE_MODEL}\n",
    "- **Conversion Date**: {datetime.now().strftime('%Y-%m-%d')}\n",
    "- **Format**: MLX\n",
    "- **Optimization**: Optimized for Apple Silicon (M1/M2/M3/M4)\n",
    "- **Quantization**: {'Yes (' + str(Q_BITS) + '-bit)' if USE_QUANTIZATION else 'No'}\n",
    "\n",
    "## Usage\n",
    "\n",
    "```python\n",
    "from mlx_lm import load, generate\n",
    "\n",
    "# Load the model\n",
    "model, tokenizer = load(\"{TARGET_REPO}\")\n",
    "\n",
    "# Generate text\n",
    "response = generate(\n",
    "    model, \n",
    "    tokenizer, \n",
    "    prompt=\"Your prompt here\", \n",
    "    max_tokens=100,\n",
    "    temp=0.7\n",
    ")\n",
    "print(response)\n",
    "```\n",
    "\n",
    "## Requirements\n",
    "\n",
    "- macOS with Apple Silicon\n",
    "- MLX framework: `pip install mlx-lm`\n",
    "\n",
    "## Performance\n",
    "\n",
    "This MLX version is optimized for Apple Silicon and should provide:\n",
    "- Faster inference on Mac devices\n",
    "- Lower memory usage\n",
    "- Better integration with macOS\n",
    "\n",
    "## License\n",
    "\n",
    "This model follows the same license as the original model: {SOURCE_MODEL}\n",
    "\n",
    "## Conversion Details\n",
    "\n",
    "- Converted using mlx-lm\n",
    "- {'Quantized to ' + str(Q_BITS) + '-bit precision' if USE_QUANTIZATION else 'No quantization applied'}\n",
    "- Tested and verified on Apple Silicon\n",
    "\"\"\"\n",
    "    \n",
    "    # Save model card\n",
    "    readme_path = mlx_model_dir / \"README.md\"\n",
    "    with open(readme_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(model_card_content)\n",
    "    \n",
    "    print(\"‚úÖ Model documentation created successfully!\")\n",
    "    print(f\"README.md saved to: {readme_path}\")\nelse:\n",
    "    print(\"Skipping documentation creation due to conversion failure.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Upload to Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if conversion_success:\n",
    "    print(f\"Preparing to upload to: {TARGET_REPO}\")\n",
    "    \n",
    "    # Create repository\n",
    "    try:\n",
    "        repo_url = api.create_repo(\n",
    "            repo_id=TARGET_REPO,\n",
    "            repo_type=\"model\",\n",
    "            exist_ok=True,\n",
    "            private=False  # Set to True if you want a private repo\n",
    "        )\n",
    "        print(f\"‚úÖ Repository {TARGET_REPO} created/confirmed!\")\n",
    "        print(f\"Repository URL: {repo_url}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Repository creation failed: {e}\")\n",
    "        print(\"Please check your repository name and permissions.\")\n",
    "        upload_success = False\n",
    "    else:\n",
    "        # Upload the model\n",
    "        print(f\"\\nUploading MLX model to {TARGET_REPO}...\")\n",
    "        print(\"This may take a while depending on model size and internet connection.\")\n",
    "        \n",
    "        try:\n",
    "            start_time = datetime.now()\n",
    "            \n",
    "            upload_folder(\n",
    "                folder_path=str(mlx_model_dir),\n",
    "                repo_id=TARGET_REPO,\n",
    "                repo_type=\"model\",\n",
    "                commit_message=f\"Add MLX-converted {SOURCE_MODEL.split('/')[-1]} model\",\n",
    "                ignore_patterns=[\".DS_Store\", \"*.pyc\", \"__pycache__\", \".git\"]\n",
    "            )\n",
    "            \n",
    "            end_time = datetime.now()\n",
    "            duration = end_time - start_time\n",
    "            \n",
    "            print(f\"\\n‚úÖ Model successfully uploaded in {duration}!\")\n",
    "            print(f\"üîó Model URL: https://huggingface.co/{TARGET_REPO}\")\n",
    "            upload_success = True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Upload failed: {e}\")\n",
    "            print(\"You may need to check your internet connection or Hugging Face permissions.\")\n",
    "            upload_success = False\nelse:\n",
    "    print(\"Skipping upload due to conversion failure.\")\n",
    "    upload_success = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11: Final Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if upload_success:\n",
    "    # Verify the upload\n",
    "    try:\n",
    "        repo_info = api.repo_info(repo_id=TARGET_REPO, repo_type=\"model\")\n",
    "        print(f\"‚úÖ Repository verified: {repo_info.id}\")\n",
    "        print(f\"üîó Repository URL: https://huggingface.co/{TARGET_REPO}\")\n",
    "        print(f\"üìÖ Last modified: {repo_info.last_modified}\")\n",
    "        \n",
    "        # List files in the repository\n",
    "        files = api.list_repo_files(repo_id=TARGET_REPO, repo_type=\"model\")\n",
    "        print(f\"\\nüìÅ Files in repository ({len(files)} total):\")\n",
    "        for file in sorted(files):\n",
    "            print(f\"  üìÑ {file}\")\n",
    "            \n",
    "        verification_success = True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Verification failed: {e}\")\n",
    "        verification_success = False\nelse:\n",
    "    print(\"Skipping verification due to upload failure.\")\n",
    "    verification_success = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 12: Cleanup Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional cleanup\n",
    "print(\"=== Cleanup Options ===\")\n",
    "print(f\"Original model location: {original_model_dir}\")\n",
    "print(f\"MLX model location: {mlx_model_dir}\")\n",
    "\n",
    "if verification_success:\n",
    "    cleanup_choice = input(\"\\nWhat would you like to clean up?\\n1. Keep all files\\n2. Delete original model only\\n3. Delete both original and MLX models\\nChoice (1-3): \").strip()\n",
    "    \n",
    "    if cleanup_choice == \"2\":\n",
    "        try:\n",
    "            shutil.rmtree(original_model_dir)\n",
    "            print(f\"‚úÖ Original model deleted: {original_model_dir}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Failed to delete original model: {e}\")\n",
    "    \n",
    "    elif cleanup_choice == \"3\":\n",
    "        try:\n",
    "            shutil.rmtree(original_model_dir)\n",
    "            shutil.rmtree(mlx_model_dir)\n",
    "            print(f\"‚úÖ Both models deleted from local storage\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Failed to delete models: {e}\")\n",
    "    \n",
    "    else:\n",
    "        print(\"‚úÖ All files kept locally\")\nelse:\n",
    "    print(\"Cleanup skipped due to incomplete conversion/upload process.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 13: Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üéâ UNIVERSAL MLX CONVERTER - FINAL REPORT\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nüìã Configuration:\")\n",
    "print(f\"   Source Model: {SOURCE_MODEL}\")\n",
    "print(f\"   Target Repository: {TARGET_REPO}\")\n",
    "print(f\"   Quantization: {'Yes (' + str(Q_BITS) + '-bit)' if USE_QUANTIZATION else 'No'}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Process Status:\")\n",
    "print(f\"   Model Download: {'‚úÖ Success' if 'downloaded_path' in locals() else '‚ùå Failed'}\")\n",
    "print(f\"   MLX Conversion: {'‚úÖ Success' if conversion_success else '‚ùå Failed'}\")\n",
    "print(f\"   Model Testing: {'‚úÖ Success' if model_test_success else '‚ùå Failed/Skipped'}\")\n",
    "print(f\"   HF Upload: {'‚úÖ Success' if upload_success else '‚ùå Failed/Skipped'}\")\n",
    "print(f\"   Verification: {'‚úÖ Success' if verification_success else '‚ùå Failed/Skipped'}\")\n",
    "\n",
    "if verification_success:\n",
    "    print(f\"\\nüîó Your converted model is available at:\")\n",
    "    print(f\"   https://huggingface.co/{TARGET_REPO}\")\n",
    "    \n",
    "    print(f\"\\nüöÄ Usage Instructions:\")\n",
    "    print(f\"   ```python\")\n",
    "    print(f\"   from mlx_lm import load, generate\")\n",
    "    print(f\"   model, tokenizer = load('{TARGET_REPO}')\")\n",
    "    print(f\"   response = generate(model, tokenizer, prompt='Hello', max_tokens=100)\")\n",
    "    print(f\"   ```\")\n",
    "\nprint(f\"\\n\" + \"=\"*60)\nprint(\"Thank you for using the Universal MLX Converter!\")\nprint(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}