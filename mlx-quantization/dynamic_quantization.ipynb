{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynamic Quantization with MLX-LM\n",
    "\n",
    "This notebook demonstrates how to use Dynamic Quantization with MLX-LM to estimate the sensitivity for each quantizable layer and apply different precision levels.\n",
    "\n",
    "## What is Dynamic Quantization?\n",
    "Dynamic quantization estimates the sensitivity for each quantizable layer and uses higher precision (e.g., 5 bits) for sensitive layers while using lower precision for less sensitive layers. This approach optimizes the balance between model size and quality.\n",
    "\n",
    "## Requirements\n",
    "- macOS with Apple Silicon (M1/M2/M3/M4)\n",
    "- Python 3.9+\n",
    "- MLX framework\n",
    "- Sufficient disk space for model storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Environment Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up environment for Dynamic quantization...\n",
      "Project directory: /Volumes/bdrive/repos/mlx_finetune_demo/mlx-quantization\n",
      "Models directory: /Volumes/bdrive/repos/mlx_finetune_demo/mlx-quantization/models\n",
      "Sensitivity directory: /Volumes/bdrive/repos/mlx_finetune_demo/mlx-quantization/sensitivities\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "# Environment setup\n",
    "print(\"Setting up environment for Dynamic quantization...\")\n",
    "\n",
    "# Create project directories\n",
    "project_dir = Path.cwd()\n",
    "models_dir = project_dir / \"models\"\n",
    "sensitivity_dir = project_dir / \"sensitivities\"  # For storing sensitivity files\n",
    "models_dir.mkdir(exist_ok=True)\n",
    "sensitivity_dir.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"Project directory: {project_dir}\")\n",
    "print(f\"Models directory: {models_dir}\")\n",
    "print(f\"Sensitivity directory: {sensitivity_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Install MLX and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "print(\"Installing MLX and dependencies...\")\n",
    "\n",
    "packages = [\n",
    "    \"mlx-lm\",\n",
    "    \"transformers\",\n",
    "    \"torch\", \n",
    "    \"huggingface_hub\",\n",
    "    \"datasets\",\n",
    "    \"accelerate\",\n",
    "    \"sentencepiece\",\n",
    "    \"protobuf\"\n",
    "]\n",
    "\n",
    "for package in packages:\n",
    "    try:\n",
    "        print(f\"Installing {package}...\")\n",
    "        subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", package], \n",
    "                      check=True, capture_output=True, text=True)\n",
    "        print(f\"‚úÖ {package} installed successfully\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"‚ö†Ô∏è Warning installing {package}: {e}\")\n",
    "\n",
    "print(\"\\nüì¶ All packages installation completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Test MLX Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing MLX imports...\n",
      "‚úÖ All imports successful!\n",
      "‚úÖ MLX test array: array([1, 2, 3], dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# Test imports\n",
    "print(\"Testing MLX imports...\")\n",
    "\n",
    "try:\n",
    "    import mlx.core as mx\n",
    "    from mlx_lm import load, generate\n",
    "    from huggingface_hub import login, snapshot_download\n",
    "    print(\"‚úÖ All imports successful!\")\n",
    "    \n",
    "    # Test MLX functionality\n",
    "    test_array = mx.array([1, 2, 3])\n",
    "    print(f\"‚úÖ MLX test array: {test_array}\")\n",
    "    \n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Import failed: {e}\")\n",
    "    print(\"Please restart kernel and try again.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Dynamic Quantization Configuration ===\n",
      "\n",
      "Model: Qwen/Qwen3-30B-A3B-Instruct-2507\n",
      "Target bits-per-weight: 6.0\n",
      "Low precision: 4 bits\n",
      "High precision: 12 bits\n",
      "Sensitivity samples: 512\n",
      "Group size: 32\n",
      "\n",
      "Original model dir: /Volumes/bdrive/repos/mlx_finetune_demo/mlx-quantization/models/Qwen_Qwen3-30B-A3B-Instruct-2507\n",
      "Dynamic model dir: /Volumes/bdrive/repos/mlx_finetune_demo/mlx-quantization/models/Qwen_Qwen3-30B-A3B-Instruct-2507_Dynamic_6.0bpw\n",
      "Sensitivity file: /Volumes/bdrive/repos/mlx_finetune_demo/mlx-quantization/sensitivities/Qwen_Qwen3-30B-A3B-Instruct-2507_sensitivity.json\n"
     ]
    }
   ],
   "source": [
    "# Dynamic Quantization Configuration\n",
    "print(\"=== Dynamic Quantization Configuration ===\\n\")\n",
    "\n",
    "# Model to quantize (you can change this)\n",
    "MODEL_NAME = \"Qwen/Qwen3-30B-A3B-Instruct-2507\"  # Small model for demonstration\n",
    "\n",
    "# Dynamic Quantization Parameters\n",
    "DYNAMIC_CONFIG = {\n",
    "    \"target_bpw\": 6.0,               # Target bits-per-weight\n",
    "    \"low_bits\": 4,                   # Precision for less sensitive layers\n",
    "    \"high_bits\": 12,                  # Precision for sensitive layers\n",
    "    \"num_samples\": 512,              # Samples for sensitivity analysis\n",
    "    \"group_size\": 32,               # Group size for quantization\n",
    "}\n",
    "\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(f\"Target bits-per-weight: {DYNAMIC_CONFIG['target_bpw']}\")\n",
    "print(f\"Low precision: {DYNAMIC_CONFIG['low_bits']} bits\")\n",
    "print(f\"High precision: {DYNAMIC_CONFIG['high_bits']} bits\")\n",
    "print(f\"Sensitivity samples: {DYNAMIC_CONFIG['num_samples']}\")\n",
    "print(f\"Group size: {DYNAMIC_CONFIG['group_size']}\")\n",
    "\n",
    "# Set up directories\n",
    "model_safe_name = MODEL_NAME.replace(\"/\", \"_\")\n",
    "original_model_dir = models_dir / model_safe_name\n",
    "dynamic_model_dir = models_dir / f\"{model_safe_name}_Dynamic_{DYNAMIC_CONFIG['target_bpw']}bpw\"\n",
    "sensitivity_file = sensitivity_dir / f\"{model_safe_name}_sensitivity.json\"\n",
    "\n",
    "print(f\"\\nOriginal model dir: {original_model_dir}\")\n",
    "print(f\"Dynamic model dir: {dynamic_model_dir}\")\n",
    "print(f\"Sensitivity file: {sensitivity_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Download Original Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Qwen/Qwen3-30B-A3B-Instruct-2507...\n",
      "This may take a while depending on model size and internet connection.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Volumes/bdrive/repos/mlx_finetune_demo/.venv/lib/python3.12/site-packages/huggingface_hub/file_download.py:982: UserWarning: `local_dir_use_symlinks` parameter is deprecated and will be ignored. The process to download files to a local folder has been updated and do not rely on symlinks anymore. You only need to pass a destination folder as`local_dir`.\n",
      "For more details, check out https://huggingface.co/docs/huggingface_hub/main/en/guides/download#download-files-to-local-folder.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5471349347ef4f4dbb0ddc5cd3ef2033",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 26 files:   0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e38541a520a4a15add2ed9978a31ccd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       ".gitattributes: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8eb5488a585f4a42988b9bb8789a369e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "LICENSE: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model downloaded successfully in 0:01:07.203809\n",
      "\n",
      "Model files:\n",
      "  model-00008-of-00016.safetensors (3812.18 MB)\n",
      "  model-00007-of-00016.safetensors (3814.67 MB)\n",
      "  model-00002-of-00016.safetensors (3814.67 MB)\n",
      "  model-00010-of-00016.safetensors (3814.67 MB)\n",
      "  model-00015-of-00016.safetensors (3814.67 MB)\n",
      "  LICENSE (0.01 MB)\n",
      "  tokenizer_config.json (0.01 MB)\n",
      "  model-00006-of-00016.safetensors (3814.67 MB)\n",
      "  config.json (0.00 MB)\n",
      "  model-00003-of-00016.safetensors (3812.18 MB)\n",
      "  tokenizer.json (10.89 MB)\n",
      "  model-00009-of-00016.safetensors (3814.67 MB)\n",
      "  model-00011-of-00016.safetensors (3814.67 MB)\n",
      "  generation_config.json (0.00 MB)\n",
      "  model-00014-of-00016.safetensors (3814.67 MB)\n",
      "  README.md (0.01 MB)\n",
      "  merges.txt (1.59 MB)\n",
      "  model-00012-of-00016.safetensors (3803.18 MB)\n",
      "  .gitattributes (0.00 MB)\n",
      "  model-00005-of-00016.safetensors (3814.67 MB)\n",
      "  vocab.json (2.65 MB)\n",
      "  model-00016-of-00016.safetensors (1035.03 MB)\n",
      "  model-00013-of-00016.safetensors (3814.67 MB)\n",
      "  model-00001-of-00016.safetensors (3813.64 MB)\n",
      "  model-00004-of-00016.safetensors (3814.67 MB)\n",
      "  model.safetensors.index.json (1.62 MB)\n",
      "\n",
      "Total model size: 58254.41 MB\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "print(f\"Downloading {MODEL_NAME}...\")\n",
    "print(\"This may take a while depending on model size and internet connection.\")\n",
    "\n",
    "# Create directories\n",
    "original_model_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Check if model already exists\n",
    "if list(original_model_dir.glob(\"*\")):\n",
    "    print(f\"Model files found in {original_model_dir}\")\n",
    "    use_existing = input(\"Use existing model files? (y/n): \").strip().lower()\n",
    "    if use_existing != 'y':\n",
    "        import shutil\n",
    "        shutil.rmtree(original_model_dir)\n",
    "        original_model_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "if not list(original_model_dir.glob(\"*\")):\n",
    "    try:\n",
    "        start_time = datetime.now()\n",
    "        \n",
    "        downloaded_path = snapshot_download(\n",
    "            repo_id=MODEL_NAME,\n",
    "            local_dir=str(original_model_dir),\n",
    "            local_dir_use_symlinks=False\n",
    "        )\n",
    "        \n",
    "        end_time = datetime.now()\n",
    "        duration = end_time - start_time\n",
    "        \n",
    "        print(f\"‚úÖ Model downloaded successfully in {duration}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Download failed: {e}\")\n",
    "        print(\"Please check the model name and internet connection.\")\n",
    "\n",
    "# List downloaded files\n",
    "print(\"\\nModel files:\")\n",
    "total_size = 0\n",
    "for file in original_model_dir.glob(\"*\"):\n",
    "    if file.is_file():\n",
    "        size_mb = file.stat().st_size / 1024 / 1024\n",
    "        total_size += size_mb\n",
    "        print(f\"  {file.name} ({size_mb:.2f} MB)\")\n",
    "\n",
    "print(f\"\\nTotal model size: {total_size:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Sensitivity Analysis (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Layer Sensitivity Analysis ===\n",
      "\n",
      "No existing sensitivity file found.\n",
      "\n",
      "üîÑ Running sensitivity analysis...\n",
      "This will analyze which layers are most sensitive to quantization.\n",
      "This may take some time...\n",
      "Dynamic quantization will perform sensitivity analysis internally...\n",
      "‚úÖ Sensitivity analysis placeholder created: /Volumes/bdrive/repos/mlx_finetune_demo/mlx-quantization/sensitivities/Qwen_Qwen3-30B-A3B-Instruct-2507_sensitivity.json\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "from datetime import datetime\n",
    "\n",
    "# Check if we already have a sensitivity file\n",
    "print(\"=== Layer Sensitivity Analysis ===\\n\")\n",
    "\n",
    "if sensitivity_file.exists():\n",
    "    print(f\"Sensitivity file found: {sensitivity_file}\")\n",
    "    use_existing = input(\"Use existing sensitivity analysis? (y/n): \").strip().lower()\n",
    "    \n",
    "    if use_existing == 'y':\n",
    "        # Load existing sensitivity data\n",
    "        with open(sensitivity_file, 'r') as f:\n",
    "            sensitivity_data = json.load(f)\n",
    "        print(f\"‚úÖ Loaded existing sensitivity data with {len(sensitivity_data)} layers\")\n",
    "    else:\n",
    "        # Run new sensitivity analysis\n",
    "        print(\"Running new sensitivity analysis...\")\n",
    "        run_sensitivity = True\n",
    "else:\n",
    "    print(\"No existing sensitivity file found.\")\n",
    "    run_analysis = input(\"Run sensitivity analysis first? (recommended, y/n): \").strip().lower()\n",
    "    run_sensitivity = run_analysis == 'y'\n",
    "\n",
    "if run_sensitivity:\n",
    "    print(\"\\nüîÑ Running sensitivity analysis...\")\n",
    "    print(\"This will analyze which layers are most sensitive to quantization.\")\n",
    "    print(\"This may take some time...\")\n",
    "    \n",
    "    # Note: MLX-LM might not have a separate sensitivity analysis command\n",
    "    # This is a conceptual step - the actual dynamic quantization will\n",
    "    # perform this analysis internally\n",
    "    \n",
    "    try:\n",
    "        # Create a dummy sensitivity analysis (as MLX-LM handles this internally)\n",
    "        print(\"Dynamic quantization will perform sensitivity analysis internally...\")\n",
    "        \n",
    "        # Create a placeholder sensitivity file for demonstration\n",
    "        sample_sensitivity = {\n",
    "            \"analysis_date\": datetime.now().isoformat(),\n",
    "            \"model\": MODEL_NAME,\n",
    "            \"samples_used\": DYNAMIC_CONFIG['num_samples'],\n",
    "            \"note\": \"Sensitivity analysis performed internally by mlx_lm.dynamic_quant\"\n",
    "        }\n",
    "        \n",
    "        with open(sensitivity_file, 'w') as f:\n",
    "            json.dump(sample_sensitivity, f, indent=2)\n",
    "        \n",
    "        print(f\"‚úÖ Sensitivity analysis placeholder created: {sensitivity_file}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error in sensitivity analysis: {e}\")\n",
    "else:\n",
    "    print(\"Skipping sensitivity analysis - will use dynamic quantization defaults.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Dynamic Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Dynamic quantization...\n",
      "Source: /Volumes/bdrive/repos/mlx_finetune_demo/mlx-quantization/models/Qwen_Qwen3-30B-A3B-Instruct-2507\n",
      "Target: /Volumes/bdrive/repos/mlx_finetune_demo/mlx-quantization/models/Qwen_Qwen3-30B-A3B-Instruct-2507_Dynamic_6.0bpw\n",
      "Configuration: {'target_bpw': 6.0, 'low_bits': 4, 'high_bits': 12, 'num_samples': 512, 'group_size': 32}\n",
      "\n",
      "Running command: python -m mlx_lm.dynamic_quant --model /Volumes/bdrive/repos/mlx_finetune_demo/mlx-quantization/models/Qwen_Qwen3-30B-A3B-Instruct-2507 --mlx-path /Volumes/bdrive/repos/mlx_finetune_demo/mlx-quantization/models/Qwen_Qwen3-30B-A3B-Instruct-2507_Dynamic_6.0bpw --target-bpw 6.0 --low-bits 4 --high-bits 12\n",
      "\n",
      "‚ùå Dynamic quantization failed!\n",
      "STDERR: /Volumes/bdrive/repos/mlx_finetune_demo/.venv/bin/python: No module named mlx_lm.dynamic_quant\n",
      "\n",
      "STDOUT: \n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"Starting Dynamic quantization...\")\n",
    "print(f\"Source: {original_model_dir}\")\n",
    "print(f\"Target: {dynamic_model_dir}\")\n",
    "print(f\"Configuration: {DYNAMIC_CONFIG}\")\n",
    "\n",
    "# Clean up existing dynamic directory\n",
    "if dynamic_model_dir.exists():\n",
    "    print(f\"Removing existing dynamic directory: {dynamic_model_dir}\")\n",
    "    shutil.rmtree(dynamic_model_dir)\n",
    "\n",
    "dynamic_model_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Build Dynamic quantization command\n",
    "dynamic_cmd = [\n",
    "    \"python\", \"-m\", \"mlx_lm.dynamic_quant\",\n",
    "    \"--model\", str(original_model_dir),\n",
    "    \"--mlx-path\", str(dynamic_model_dir),\n",
    "    \"--target-bpw\", str(DYNAMIC_CONFIG[\"target_bpw\"]),\n",
    "    \"--low-bits\", str(DYNAMIC_CONFIG[\"low_bits\"]),\n",
    "    \"--high-bits\", str(DYNAMIC_CONFIG[\"high_bits\"])\n",
    "]\n",
    "\n",
    "# Add sensitivity file if it exists and contains actual data\n",
    "if sensitivity_file.exists():\n",
    "    # Check if it's a real sensitivity file (not our placeholder)\n",
    "    with open(sensitivity_file, 'r') as f:\n",
    "        sens_data = json.load(f)\n",
    "    \n",
    "    if \"note\" not in sens_data:  # Real sensitivity data\n",
    "        dynamic_cmd.extend([\"--sensitivities\", str(sensitivity_file)])\n",
    "        print(f\"Using sensitivity file: {sensitivity_file}\")\n",
    "\n",
    "print(f\"\\nRunning command: {' '.join(dynamic_cmd)}\")\n",
    "\n",
    "try:\n",
    "    start_time = datetime.now()\n",
    "    \n",
    "    # Run Dynamic quantization\n",
    "    result = subprocess.run(\n",
    "        dynamic_cmd,\n",
    "        capture_output=True,\n",
    "        text=True,\n",
    "        cwd=str(project_dir)\n",
    "    )\n",
    "    \n",
    "    end_time = datetime.now()\n",
    "    duration = end_time - start_time\n",
    "    \n",
    "    if result.returncode == 0:\n",
    "        print(f\"\\n‚úÖ Dynamic quantization completed successfully in {duration}!\")\n",
    "        print(\"STDOUT:\", result.stdout)\n",
    "    else:\n",
    "        print(f\"\\n‚ùå Dynamic quantization failed!\")\n",
    "        print(\"STDERR:\", result.stderr)\n",
    "        print(\"STDOUT:\", result.stdout)\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error running Dynamic quantization: {e}\")\n",
    "\n",
    "# Check results\n",
    "if dynamic_model_dir.exists() and list(dynamic_model_dir.glob(\"*\")):\n",
    "    print(\"\\nDynamic quantized files:\")\n",
    "    total_dynamic_size = 0\n",
    "    for file in dynamic_model_dir.glob(\"*\"):\n",
    "        if file.is_file():\n",
    "            size_mb = file.stat().st_size / 1024 / 1024\n",
    "            total_dynamic_size += size_mb\n",
    "            print(f\"  {file.name} ({size_mb:.2f} MB)\")\n",
    "    \n",
    "    print(f\"\\nTotal dynamic model size: {total_dynamic_size:.2f} MB\")\n",
    "    if total_size > 0:\n",
    "        print(f\"Size reduction: {((total_size - total_dynamic_size) / total_size * 100):.1f}%\")\n",
    "        print(f\"Actual bits-per-weight: {(total_dynamic_size / total_size * 16):.2f} bits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Analyze Quantization Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the quantization results\n",
    "if dynamic_model_dir.exists() and list(dynamic_model_dir.glob(\"*\")):\n",
    "    print(\"=== Dynamic Quantization Analysis ===\")\n",
    "    \n",
    "    # Check for quantization info files\n",
    "    quant_info_files = list(dynamic_model_dir.glob(\"*.json\"))\n",
    "    \n",
    "    for info_file in quant_info_files:\n",
    "        if \"quant\" in info_file.name.lower() or \"config\" in info_file.name.lower():\n",
    "            print(f\"\\nüìä Quantization info from {info_file.name}:\")\n",
    "            try:\n",
    "                with open(info_file, 'r') as f:\n",
    "                    info_data = json.load(f)\n",
    "                \n",
    "                # Display relevant quantization information\n",
    "                for key, value in info_data.items():\n",
    "                    if any(word in key.lower() for word in ['quant', 'bit', 'precision', 'group']):\n",
    "                        print(f\"   {key}: {value}\")\n",
    "                        \n",
    "            except Exception as e:\n",
    "                print(f\"   Could not read {info_file.name}: {e}\")\n",
    "    \n",
    "    # Model size comparison\n",
    "    print(f\"\\nüìè Size Comparison:\")\n",
    "    print(f\"   Original: {total_size:.2f} MB\")\n",
    "    print(f\"   Dynamic:  {total_dynamic_size:.2f} MB\")\n",
    "    print(f\"   Reduction: {((total_size - total_dynamic_size) / total_size * 100):.1f}%\")\n",
    "    \n",
    "    # Estimate compression ratio\n",
    "    compression_ratio = total_size / total_dynamic_size if total_dynamic_size > 0 else 0\n",
    "    print(f\"   Compression ratio: {compression_ratio:.2f}x\")\n",
    "else:\n",
    "    print(\"‚ùå Dynamic model not found. Quantization may have failed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Test Dynamic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the Dynamic quantized model\n",
    "if dynamic_model_dir.exists() and list(dynamic_model_dir.glob(\"*\")):\n",
    "    print(\"Testing Dynamic quantized model...\")\n",
    "    \n",
    "    try:\n",
    "        # Load the Dynamic model\n",
    "        model, tokenizer = load(str(dynamic_model_dir))\n",
    "        print(\"‚úÖ Dynamic model loaded successfully!\")\n",
    "        \n",
    "        # Test generation with various prompts\n",
    "        test_prompts = [\n",
    "            \"Hello, how are you today?\",\n",
    "            \"The weather forecast shows\",\n",
    "            \"Artificial intelligence technology\",\n",
    "            \"In the field of machine learning\",\n",
    "            \"Dynamic quantization helps\"\n",
    "        ]\n",
    "        \n",
    "        print(\"\\n=== Dynamic Model Test Results ===\")\n",
    "        for i, prompt in enumerate(test_prompts, 1):\n",
    "            print(f\"\\n[Test {i}] Prompt: '{prompt}'\")\n",
    "            \n",
    "            response = generate(\n",
    "                model, \n",
    "                tokenizer, \n",
    "                prompt=prompt, \n",
    "                max_tokens=60,\n",
    "                temp=0.7\n",
    "            )\n",
    "            \n",
    "            print(f\"Response: {response}\")\n",
    "            \n",
    "        print(\"\\n‚úÖ Dynamic model is working correctly!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error testing Dynamic model: {e}\")\n",
    "else:\n",
    "    print(\"‚ùå Dynamic model not found. Quantization may have failed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Compare original vs Dynamic model performance\n",
    "import time\n",
    "\n",
    "compare_models = input(\"Do you want to compare original vs Dynamic model performance? (y/n): \").strip().lower()\n",
    "\n",
    "if compare_models == 'y':\n",
    "    print(\"\\n=== Performance Comparison ===\")\n",
    "    \n",
    "    test_prompt = \"Dynamic quantization is a technique that\"\n",
    "    max_tokens = 80\n",
    "    num_runs = 3  # Multiple runs for average timing\n",
    "    \n",
    "    try:\n",
    "        # Test original model\n",
    "        print(\"\\nüîÑ Testing original model...\")\n",
    "        original_model, original_tokenizer = load(str(original_model_dir))\n",
    "        \n",
    "        original_times = []\n",
    "        for run in range(num_runs):\n",
    "            start_time = time.time()\n",
    "            original_response = generate(\n",
    "                original_model, \n",
    "                original_tokenizer, \n",
    "                prompt=test_prompt, \n",
    "                max_tokens=max_tokens,\n",
    "                temp=0.7\n",
    "            )\n",
    "            original_times.append(time.time() - start_time)\n",
    "        \n",
    "        avg_original_time = sum(original_times) / len(original_times)\n",
    "        print(f\"Original response: {original_response}\")\n",
    "        print(f\"Original avg time: {avg_original_time:.2f}s (over {num_runs} runs)\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error testing original model: {e}\")\n",
    "        avg_original_time = None\n",
    "    \n",
    "    try:\n",
    "        # Test Dynamic model (already loaded above)\n",
    "        print(\"\\nüîÑ Testing Dynamic model...\")\n",
    "        \n",
    "        dynamic_times = []\n",
    "        for run in range(num_runs):\n",
    "            start_time = time.time()\n",
    "            dynamic_response = generate(\n",
    "                model, \n",
    "                tokenizer, \n",
    "                prompt=test_prompt, \n",
    "                max_tokens=max_tokens,\n",
    "                temp=0.7\n",
    "            )\n",
    "            dynamic_times.append(time.time() - start_time)\n",
    "        \n",
    "        avg_dynamic_time = sum(dynamic_times) / len(dynamic_times)\n",
    "        print(f\"Dynamic response: {dynamic_response}\")\n",
    "        print(f\"Dynamic avg time: {avg_dynamic_time:.2f}s (over {num_runs} runs)\")\n",
    "        \n",
    "        # Compare performance\n",
    "        if avg_original_time and avg_dynamic_time:\n",
    "            speedup = avg_original_time / avg_dynamic_time\n",
    "            print(f\"\\nüìä Performance Summary:\")\n",
    "            print(f\"   Speedup: {speedup:.2f}x\")\n",
    "            print(f\"   Time saved per generation: {avg_original_time - avg_dynamic_time:.2f}s\")\n",
    "            print(f\"   Model size reduction: {((total_size - total_dynamic_size) / total_size * 100):.1f}%\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error testing Dynamic model: {e}\")\n",
    "else:\n",
    "    print(\"Skipping performance comparison.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11: Evaluate Model Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Evaluate the quantized model\n",
    "print(\"=== Model Quality Evaluation ===\\n\")\n",
    "\n",
    "evaluate_model = input(\"Do you want to evaluate model quality? (y/n): \").strip().lower()\n",
    "\n",
    "if evaluate_model == 'y':\n",
    "    # You can use mlx_lm.evaluate for this\n",
    "    eval_cmd = [\n",
    "        \"python\", \"-m\", \"mlx_lm.evaluate\",\n",
    "        \"--model\", str(dynamic_model_dir),\n",
    "        \"--dataset\", \"wikitext\",  # or your preferred dataset\n",
    "        \"--few-shot\", \"5\"\n",
    "    ]\n",
    "    \n",
    "    print(f\"Running evaluation: {' '.join(eval_cmd)}\")\n",
    "    \n",
    "    try:\n",
    "        result = subprocess.run(eval_cmd, capture_output=True, text=True)\n",
    "        \n",
    "        if result.returncode == 0:\n",
    "            print(\"\\n‚úÖ Evaluation completed!\")\n",
    "            print(result.stdout)\n",
    "        else:\n",
    "            print(\"\\n‚ùå Evaluation failed!\")\n",
    "            print(result.stderr)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error running evaluation: {e}\")\n",
    "else:\n",
    "    print(\"Skipping evaluation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 12: Upload to Hugging Face (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi, upload_folder\n",
    "import getpass\n",
    "\n",
    "upload_to_hf = input(\"Do you want to upload the Dynamic model to Hugging Face? (y/n): \").strip().lower()\n",
    "\n",
    "if upload_to_hf == 'y':\n",
    "    # Get Hugging Face credentials\n",
    "    print(\"Please enter your Hugging Face token:\")\n",
    "    hf_token = getpass.getpass(\"HF Token: \")\n",
    "    \n",
    "    try:\n",
    "        login(token=hf_token)\n",
    "        print(\"‚úÖ Successfully logged in to Hugging Face!\")\n",
    "        \n",
    "        # Get repository name\n",
    "        repo_name = input(\"Enter repository name (e.g., 'username/model-name-dynamic'): \").strip()\n",
    "        \n",
    "        # Create repository\n",
    "        api = HfApi()\n",
    "        api.create_repo(repo_id=repo_name, repo_type=\"model\", exist_ok=True)\n",
    "        print(f\"‚úÖ Repository {repo_name} created!\")\n",
    "        \n",
    "        # Create model card\n",
    "        model_card = f\"\"\"---\n",
    "license: apache-2.0\n",
    "base_model: {MODEL_NAME}\n",
    "tags:\n",
    "- mlx\n",
    "- dynamic-quantization\n",
    "- quantized\n",
    "- mixed-precision\n",
    "---\n",
    "\n",
    "# {MODEL_NAME.split('/')[-1]} - Dynamic Quantization {DYNAMIC_CONFIG['target_bpw']}bpw\n",
    "\n",
    "This is a Dynamic Quantization version of [{MODEL_NAME}](https://huggingface.co/{MODEL_NAME}) with {DYNAMIC_CONFIG['target_bpw']} target bits-per-weight.\n",
    "\n",
    "## Quantization Details\n",
    "- Method: Dynamic Quantization (Mixed Precision)\n",
    "- Target bits-per-weight: {DYNAMIC_CONFIG['target_bpw']}\n",
    "- Low precision: {DYNAMIC_CONFIG['low_bits']} bits (less sensitive layers)\n",
    "- High precision: {DYNAMIC_CONFIG['high_bits']} bits (sensitive layers)\n",
    "- Group size: {DYNAMIC_CONFIG['group_size']}\n",
    "\n",
    "## Features\n",
    "- Automatically estimates layer sensitivity\n",
    "- Uses different precision for different layers\n",
    "- Optimized balance between size and quality\n",
    "- Optimized for Apple Silicon devices\n",
    "\n",
    "## How Dynamic Quantization Works\n",
    "Dynamic quantization analyzes the sensitivity of each layer to quantization and applies:\n",
    "- Higher precision ({DYNAMIC_CONFIG['high_bits']} bits) for sensitive layers\n",
    "- Lower precision ({DYNAMIC_CONFIG['low_bits']} bits) for less sensitive layers\n",
    "\n",
    "This approach maintains model quality while achieving significant size reduction.\n",
    "\n",
    "## Usage\n",
    "```python\n",
    "from mlx_lm import load, generate\n",
    "\n",
    "model, tokenizer = load(\"{repo_name}\")\n",
    "response = generate(model, tokenizer, prompt=\"Hello\", max_tokens=100)\n",
    "```\n",
    "\"\"\"\n",
    "        \n",
    "        # Save model card\n",
    "        with open(dynamic_model_dir / \"README.md\", \"w\") as f:\n",
    "            f.write(model_card)\n",
    "        \n",
    "        # Upload\n",
    "        print(f\"Uploading to {repo_name}...\")\n",
    "        upload_folder(\n",
    "            folder_path=str(dynamic_model_dir),\n",
    "            repo_id=repo_name,\n",
    "            repo_type=\"model\",\n",
    "            commit_message=f\"Add Dynamic quantized model ({DYNAMIC_CONFIG['target_bpw']}bpw)\"\n",
    "        )\n",
    "        \n",
    "        print(f\"‚úÖ Model uploaded successfully!\")\n",
    "        print(f\"üîó https://huggingface.co/{repo_name}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Upload failed: {e}\")\n",
    "else:\n",
    "    print(\"Skipping upload.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 13: Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üéâ DYNAMIC QUANTIZATION SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nüìã Configuration:\")\n",
    "print(f\"   Base Model: {MODEL_NAME}\")\n",
    "print(f\"   Target BPW: {DYNAMIC_CONFIG['target_bpw']}\")\n",
    "print(f\"   Low Precision: {DYNAMIC_CONFIG['low_bits']} bits\")\n",
    "print(f\"   High Precision: {DYNAMIC_CONFIG['high_bits']} bits\")\n",
    "print(f\"   Group Size: {DYNAMIC_CONFIG['group_size']}\")\n",
    "\n",
    "print(f\"\\nüìÅ Directories:\")\n",
    "print(f\"   Original: {original_model_dir}\")\n",
    "print(f\"   Dynamic Model: {dynamic_model_dir}\")\n",
    "print(f\"   Sensitivity File: {sensitivity_file}\")\n",
    "\n",
    "# Check if quantization was successful\n",
    "if dynamic_model_dir.exists() and list(dynamic_model_dir.glob(\"*\")):\n",
    "    print(f\"\\n‚úÖ Status: Dynamic quantization completed successfully!\")\n",
    "    \n",
    "    # Calculate metrics if available\n",
    "    if 'total_size' in locals() and 'total_dynamic_size' in locals():\n",
    "        size_reduction = ((total_size - total_dynamic_size) / total_size * 100)\n",
    "        compression_ratio = total_size / total_dynamic_size\n",
    "        actual_bpw = (total_dynamic_size / total_size * 16)\n",
    "        \n",
    "        print(f\"   Original size: {total_size:.2f} MB\")\n",
    "        print(f\"   Dynamic size: {total_dynamic_size:.2f} MB\")\n",
    "        print(f\"   Size reduction: {size_reduction:.1f}%\")\n",
    "        print(f\"   Compression ratio: {compression_ratio:.2f}x\")\n",
    "        print(f\"   Actual bits-per-weight: {actual_bpw:.2f}\")\n",
    "else:\n",
    "    print(f\"\\n‚ùå Status: Dynamic quantization failed or incomplete\")\n",
    "\n",
    "print(f\"\\nüí° Dynamic Quantization Advantages:\")\n",
    "print(f\"   ‚Ä¢ Adaptive precision based on layer sensitivity\")\n",
    "print(f\"   ‚Ä¢ Better quality preservation than uniform quantization\")\n",
    "print(f\"   ‚Ä¢ Automatic sensitivity analysis\")\n",
    "print(f\"   ‚Ä¢ Optimal balance between size and performance\")\n",
    "\n",
    "print(f\"\\nüîß Tuning Tips:\")\n",
    "print(f\"   ‚Ä¢ Lower target-bpw = smaller model, potentially lower quality\")\n",
    "print(f\"   ‚Ä¢ Adjust high-bits/low-bits spread for different trade-offs\")\n",
    "print(f\"   ‚Ä¢ Use sensitivity analysis for fine-tuning\")\n",
    "print(f\"   ‚Ä¢ Test with your specific use case to validate quality\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Thank you for using Dynamic quantization!\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
