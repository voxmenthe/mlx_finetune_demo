{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DWQ (Distilled Weight Quantization) with MLX-LM\n",
    "\n",
    "This notebook demonstrates how to use DWQ (Distilled Weight Quantization) with MLX-LM to reduce model quality loss during quantization.\n",
    "\n",
    "## What is DWQ?\n",
    "DWQ is designed to minimize quality loss when quantizing models to lower bit precision. It works best for 2-4 bit models and uses calibration samples to maintain model performance.\n",
    "\n",
    "## Requirements\n",
    "- macOS with Apple Silicon (M1/M2/M3/M4)\n",
    "- Python 3.9+\n",
    "- MLX framework\n",
    "- Sufficient disk space for model storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Environment Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "# Environment setup\n",
    "print(\"Setting up environment for DWQ quantization...\")\n",
    "\n",
    "# Create project directories\n",
    "project_dir = Path.cwd()\n",
    "models_dir = project_dir / \"models\"\n",
    "models_dir.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"Project directory: {project_dir}\")\n",
    "print(f\"Models directory: {models_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Install MLX and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "print(\"Installing MLX and dependencies...\")\n",
    "\n",
    "packages = [\n",
    "    \"mlx-lm\",\n",
    "    \"transformers\",\n",
    "    \"torch\", \n",
    "    \"huggingface_hub\",\n",
    "    \"datasets\",\n",
    "    \"accelerate\",\n",
    "    \"sentencepiece\",\n",
    "    \"protobuf\"\n",
    "]\n",
    "\n",
    "for package in packages:\n",
    "    try:\n",
    "        print(f\"Installing {package}...\")\n",
    "        subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", package], \n",
    "                      check=True, capture_output=True, text=True)\n",
    "        print(f\"‚úÖ {package} installed successfully\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"‚ö†Ô∏è Warning installing {package}: {e}\")\n",
    "\n",
    "print(\"\\nüì¶ All packages installation completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Test MLX Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test imports\n",
    "print(\"Testing MLX imports...\")\n",
    "\n",
    "try:\n",
    "    import mlx.core as mx\n",
    "    from mlx_lm import load, generate\n",
    "    from huggingface_hub import login, snapshot_download\n",
    "    print(\"‚úÖ All imports successful!\")\n",
    "    \n",
    "    # Test MLX functionality\n",
    "    test_array = mx.array([1, 2, 3])\n",
    "    print(f\"‚úÖ MLX test array: {test_array}\")\n",
    "    \n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Import failed: {e}\")\n",
    "    print(\"Please restart kernel and try again.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DWQ Configuration\n",
    "print(\"=== DWQ Configuration ===\\n\")\n",
    "\n",
    "# Model to quantize (you can change this)\n",
    "MODEL_NAME = \"Qwen/Qwen2.5-0.5B\"  # Small model for demonstration\n",
    "\n",
    "# DWQ Parameters\n",
    "DWQ_CONFIG = {\n",
    "    \"bits\": 4,                    # Quantization precision (2-4 bits work best)\n",
    "    \"num_samples\": 1024,          # Calibration samples (default: 1024)\n",
    "    \"batch_size\": 8,              # Batch size to reduce memory footprint\n",
    "    \"group_size\": 64,             # Group size (smaller can improve results)\n",
    "    \"learning_rate\": 0.01,        # Learning rate (adjust based on precision)\n",
    "}\n",
    "\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(f\"Target bits: {DWQ_CONFIG['bits']}\")\n",
    "print(f\"Calibration samples: {DWQ_CONFIG['num_samples']}\")\n",
    "print(f\"Batch size: {DWQ_CONFIG['batch_size']}\")\n",
    "print(f\"Group size: {DWQ_CONFIG['group_size']}\")\n",
    "\n",
    "# Set up directories\n",
    "original_model_dir = models_dir / MODEL_NAME.replace(\"/\", \"_\")\n",
    "dwq_model_dir = models_dir / f\"{MODEL_NAME.replace('/', '_')}_DWQ_{DWQ_CONFIG['bits']}bit\"\n",
    "\n",
    "print(f\"\\nOriginal model dir: {original_model_dir}\")\n",
    "print(f\"DWQ model dir: {dwq_model_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Download Original Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "print(f\"Downloading {MODEL_NAME}...\")\n",
    "print(\"This may take a while depending on model size and internet connection.\")\n",
    "\n",
    "# Create directories\n",
    "original_model_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Check if model already exists\n",
    "if list(original_model_dir.glob(\"*\")):\n",
    "    print(f\"Model files found in {original_model_dir}\")\n",
    "    use_existing = input(\"Use existing model files? (y/n): \").strip().lower()\n",
    "    if use_existing != 'y':\n",
    "        import shutil\n",
    "        shutil.rmtree(original_model_dir)\n",
    "        original_model_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "if not list(original_model_dir.glob(\"*\")):\n",
    "    try:\n",
    "        start_time = datetime.now()\n",
    "        \n",
    "        downloaded_path = snapshot_download(\n",
    "            repo_id=MODEL_NAME,\n",
    "            local_dir=str(original_model_dir),\n",
    "            local_dir_use_symlinks=False\n",
    "        )\n",
    "        \n",
    "        end_time = datetime.now()\n",
    "        duration = end_time - start_time\n",
    "        \n",
    "        print(f\"‚úÖ Model downloaded successfully in {duration}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Download failed: {e}\")\n",
    "        print(\"Please check the model name and internet connection.\")\n",
    "\n",
    "# List downloaded files\n",
    "print(\"\\nModel files:\")\n",
    "total_size = 0\n",
    "for file in original_model_dir.glob(\"*\"):\n",
    "    if file.is_file():\n",
    "        size_mb = file.stat().st_size / 1024 / 1024\n",
    "        total_size += size_mb\n",
    "        print(f\"  {file.name} ({size_mb:.2f} MB)\")\n",
    "\n",
    "print(f\"\\nTotal model size: {total_size:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: DWQ Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"Starting DWQ quantization...\")\n",
    "print(f\"Source: {original_model_dir}\")\n",
    "print(f\"Target: {dwq_model_dir}\")\n",
    "print(f\"Configuration: {DWQ_CONFIG}\")\n",
    "\n",
    "# Clean up existing DWQ directory\n",
    "if dwq_model_dir.exists():\n",
    "    print(f\"Removing existing DWQ directory: {dwq_model_dir}\")\n",
    "    shutil.rmtree(dwq_model_dir)\n",
    "\n",
    "dwq_model_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Build DWQ command\n",
    "dwq_cmd = [\n",
    "    \"python\", \"-m\", \"mlx_lm.dwq\",\n",
    "    \"--model\", str(original_model_dir),\n",
    "    \"--mlx-path\", str(dwq_model_dir),\n",
    "    \"--bits\", str(DWQ_CONFIG[\"bits\"]),\n",
    "    \"--num-samples\", str(DWQ_CONFIG[\"num_samples\"]),\n",
    "    \"--batch-size\", str(DWQ_CONFIG[\"batch_size\"])\n",
    "]\n",
    "\n",
    "print(f\"\\nRunning command: {' '.join(dwq_cmd)}\")\n",
    "\n",
    "try:\n",
    "    start_time = datetime.now()\n",
    "    \n",
    "    # Run DWQ quantization\n",
    "    result = subprocess.run(\n",
    "        dwq_cmd,\n",
    "        capture_output=True,\n",
    "        text=True,\n",
    "        cwd=str(project_dir)\n",
    "    )\n",
    "    \n",
    "    end_time = datetime.now()\n",
    "    duration = end_time - start_time\n",
    "    \n",
    "    if result.returncode == 0:\n",
    "        print(f\"\\n‚úÖ DWQ quantization completed successfully in {duration}!\")\n",
    "        print(\"STDOUT:\", result.stdout)\n",
    "    else:\n",
    "        print(f\"\\n‚ùå DWQ quantization failed!\")\n",
    "        print(\"STDERR:\", result.stderr)\n",
    "        print(\"STDOUT:\", result.stdout)\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error running DWQ: {e}\")\n",
    "\n",
    "# Check results\n",
    "if dwq_model_dir.exists() and list(dwq_model_dir.glob(\"*\")):\n",
    "    print(\"\\nDWQ quantized files:\")\n",
    "    total_size = 0\n",
    "    for file in dwq_model_dir.glob(\"*\"):\n",
    "        if file.is_file():\n",
    "            size_mb = file.stat().st_size / 1024 / 1024\n",
    "            total_size += size_mb\n",
    "            print(f\"  {file.name} ({size_mb:.2f} MB)\")\n",
    "    \n",
    "    print(f\"\\nTotal DWQ model size: {total_size:.2f} MB\")\n",
    "    print(f\"Size reduction: {((total_size/total_size if 'total_size' in locals() else 0) - 1) * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Test DWQ Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the DWQ quantized model\n",
    "if dwq_model_dir.exists() and list(dwq_model_dir.glob(\"*\")):\n",
    "    print(\"Testing DWQ quantized model...\")\n",
    "    \n",
    "    try:\n",
    "        # Load the DWQ model\n",
    "        model, tokenizer = load(str(dwq_model_dir))\n",
    "        print(\"‚úÖ DWQ model loaded successfully!\")\n",
    "        \n",
    "        # Test generation\n",
    "        test_prompts = [\n",
    "            \"Hello, how are you?\",\n",
    "            \"The weather today is\",\n",
    "            \"Artificial intelligence is\"\n",
    "        ]\n",
    "        \n",
    "        print(\"\\n=== DWQ Model Test Results ===\")\n",
    "        for prompt in test_prompts:\n",
    "            print(f\"\\nPrompt: '{prompt}'\")\n",
    "            \n",
    "            response = generate(\n",
    "                model, \n",
    "                tokenizer, \n",
    "                prompt=prompt, \n",
    "                max_tokens=50,\n",
    "                temp=0.7\n",
    "            )\n",
    "            \n",
    "            print(f\"Response: {response}\")\n",
    "            \n",
    "        print(\"\\n‚úÖ DWQ model is working correctly!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error testing DWQ model: {e}\")\nelse:\n",
    "    print(\"‚ùå DWQ model not found. Quantization may have failed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Evaluate Model Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Evaluate the quantized model\n",
    "# This requires a dataset for evaluation\n",
    "\n",
    "print(\"=== Model Quality Evaluation ===\\n\")\n",
    "\n",
    "evaluate_model = input(\"Do you want to evaluate model quality? (y/n): \").strip().lower()\n",
    "\n",
    "if evaluate_model == 'y':\n",
    "    # You can use mlx_lm.evaluate for this\n",
    "    eval_cmd = [\n",
    "        \"python\", \"-m\", \"mlx_lm.evaluate\",\n",
    "        \"--model\", str(dwq_model_dir),\n",
    "        \"--dataset\", \"wikitext\",  # or your preferred dataset\n",
    "        \"--few-shot\", \"5\"\n",
    "    ]\n",
    "    \n",
    "    print(f\"Running evaluation: {' '.join(eval_cmd)}\")\n",
    "    \n",
    "    try:\n",
    "        result = subprocess.run(eval_cmd, capture_output=True, text=True)\n",
    "        \n",
    "        if result.returncode == 0:\n",
    "            print(\"\\n‚úÖ Evaluation completed!\")\n",
    "            print(result.stdout)\n",
    "        else:\n",
    "            print(\"\\n‚ùå Evaluation failed!\")\n",
    "            print(result.stderr)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error running evaluation: {e}\")\nelse:\n",
    "    print(\"Skipping evaluation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Upload to Hugging Face (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi, upload_folder\n",
    "import getpass\n",
    "\n",
    "upload_to_hf = input(\"Do you want to upload the DWQ model to Hugging Face? (y/n): \").strip().lower()\n",
    "\n",
    "if upload_to_hf == 'y':\n",
    "    # Get Hugging Face credentials\n",
    "    print(\"Please enter your Hugging Face token:\")\n",
    "    hf_token = getpass.getpass(\"HF Token: \")\n",
    "    \n",
    "    try:\n",
    "        login(token=hf_token)\n",
    "        print(\"‚úÖ Successfully logged in to Hugging Face!\")\n",
    "        \n",
    "        # Get repository name\n",
    "        repo_name = input(\"Enter repository name (e.g., 'username/model-name-dwq'): \").strip()\n",
    "        \n",
    "        # Create repository\n",
    "        api = HfApi()\n",
    "        api.create_repo(repo_id=repo_name, repo_type=\"model\", exist_ok=True)\n",
    "        print(f\"‚úÖ Repository {repo_name} created!\")\n",
    "        \n",
    "        # Create model card\n",
    "        model_card = f\"\"\"---\n",
    "license: apache-2.0\n",
    "base_model: {MODEL_NAME}\n",
    "tags:\n",
    "- mlx\n",
    "- dwq\n",
    "- quantized\n",
    "- {DWQ_CONFIG['bits']}-bit\n",
    "---\n",
    "\n",
    "# {MODEL_NAME.split('/')[-1]} - DWQ {DWQ_CONFIG['bits']}-bit\n",
    "\n",
    "This is a DWQ (Distilled Weight Quantization) {DWQ_CONFIG['bits']}-bit quantized version of [{MODEL_NAME}](https://huggingface.co/{MODEL_NAME}).\n",
    "\n",
    "## Quantization Details\n",
    "- Method: DWQ (Distilled Weight Quantization)\n",
    "- Precision: {DWQ_CONFIG['bits']}-bit\n",
    "- Calibration samples: {DWQ_CONFIG['num_samples']}\n",
    "- Group size: {DWQ_CONFIG['group_size']}\n",
    "\n",
    "## Usage\n",
    "```python\n",
    "from mlx_lm import load, generate\n",
    "\n",
    "model, tokenizer = load(\"{repo_name}\")\n",
    "response = generate(model, tokenizer, prompt=\"Hello\", max_tokens=100)\n",
    "```\n",
    "\"\"\"\n",
    "        \n",
    "        # Save model card\n",
    "        with open(dwq_model_dir / \"README.md\", \"w\") as f:\n",
    "            f.write(model_card)\n",
    "        \n",
    "        # Upload\n",
    "        print(f\"Uploading to {repo_name}...\")\n",
    "        upload_folder(\n",
    "            folder_path=str(dwq_model_dir),\n",
    "            repo_id=repo_name,\n",
    "            repo_type=\"model\",\n",
    "            commit_message=f\"Add DWQ {DWQ_CONFIG['bits']}-bit quantized model\"\n",
    "        )\n",
    "        \n",
    "        print(f\"‚úÖ Model uploaded successfully!\")\n",
    "        print(f\"üîó https://huggingface.co/{repo_name}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Upload failed: {e}\")\nelse:\n",
    "    print(\"Skipping upload.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üéâ DWQ QUANTIZATION SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nüìã Configuration:\")\n",
    "print(f\"   Base Model: {MODEL_NAME}\")\n",
    "print(f\"   Target Bits: {DWQ_CONFIG['bits']}\")\n",
    "print(f\"   Calibration Samples: {DWQ_CONFIG['num_samples']}\")\n",
    "print(f\"   Group Size: {DWQ_CONFIG['group_size']}\")\n",
    "\n",
    "print(f\"\\nüìÅ Directories:\")\n",
    "print(f\"   Original: {original_model_dir}\")\n",
    "print(f\"   DWQ Model: {dwq_model_dir}\")\n",
    "\n",
    "# Check if quantization was successful\n",
    "if dwq_model_dir.exists() and list(dwq_model_dir.glob(\"*\")):\n",
    "    print(f\"\\n‚úÖ Status: DWQ quantization completed successfully!\")\n",
    "    \n",
    "    # Calculate size reduction if possible\n",
    "    original_size = sum(f.stat().st_size for f in original_model_dir.glob(\"*\") if f.is_file()) / 1024 / 1024\n",
    "    dwq_size = sum(f.stat().st_size for f in dwq_model_dir.glob(\"*\") if f.is_file()) / 1024 / 1024\n",
    "    \n",
    "    print(f\"   Original size: {original_size:.2f} MB\")\n",
    "    print(f\"   DWQ size: {dwq_size:.2f} MB\")\n",
    "    print(f\"   Size reduction: {((original_size - dwq_size) / original_size * 100):.1f}%\")\nelse:\n",
    "    print(f\"\\n‚ùå Status: DWQ quantization failed or incomplete\")\n",
    "\n",
    "print(f\"\\nüí° Tips for DWQ:\")\n",
    "print(f\"   ‚Ä¢ Works best for 2-4 bit quantization\")\n",
    "print(f\"   ‚Ä¢ Decreasing group size can improve results\")\n",
    "print(f\"   ‚Ä¢ Adjust learning rate based on precision\")\n",
    "print(f\"   ‚Ä¢ More calibration samples = better quality\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Thank you for using DWQ quantization!\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}