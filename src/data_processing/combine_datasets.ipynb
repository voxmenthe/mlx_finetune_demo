{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69fb2a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "merge_high_quality_sft.py\n",
    "Download, filter (min length), merge and upload several SFT datasets.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from typing import List, Dict, Any\n",
    "import datasets as ds\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# CONFIGURATION\n",
    "# ------------------------------------------------------------------\n",
    "HF_TOKEN    = os.getenv(\"HF_TOKEN\")          # Hugging Face token (write scope)\n",
    "OUTPUT_REPO = \"YOUR_HF_USERNAME/merged-sft-mix\"\n",
    "MIN_TOKENS  = 64                             # min prompt+answer tokens\n",
    "TOKENIZER   = \"meta-llama/Llama-3.1-8B-Instruct\"  # fast, permissive tokenizer\n",
    "\n",
    "DATASET_SPECS = {\n",
    "    \"tulu3\":          \"allenai/tulu-3-sft-mixture\", # 939,343 rows 1.41GB\n",
    "    \"hermes3\":        \"NousResearch/Hermes-3-Dataset\",\n",
    "    \"perfectblend\":   \"mlabonne/open-perfectblend\",\n",
    "    \"acereason\":      \"nvidia/AceReason-1.1-SFT\",\n",
    "    \"moaa\":           \"togethercomputer/gemma-2-9b-it-MoAA-DPO\",  # DPO pairs → use chosen\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b296690d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a2be1b4c28240afa8b336125df10ab1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc01e67de23f4cbc9c95330d5475bb11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00000-of-00006.parquet:   0%|          | 0.00/249M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88ea6d27339f430f9585dabf68764153",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00001-of-00006.parquet:   0%|          | 0.00/248M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cab904b9341b47b4981197ec691a958c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00002-of-00006.parquet:   0%|          | 0.00/247M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e4ebbfafa8c460997272592217165e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00003-of-00006.parquet:   0%|          | 0.00/248M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aba3d564965341099d49a3f08ed8da33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00004-of-00006.parquet:   0%|          | 0.00/247M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f710b748c1a443938608e4eadeebe807",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00005-of-00006.parquet:   0%|          | 0.00/246M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29dd9bd0c7c246839008066e316149fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/1420909 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "d = ds.load_dataset(\"mlabonne/open-perfectblend\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "37f9e417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1420909\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'conversations': [{'from': 'human',\n",
       "   'value': 'Augment this coding dilemma to handle not just tuples laden with duos, repetitions and idiosyncratic numbers of elements, but to precisely calculate the median devoid of classification or employing pre-existing operations. This enhanced code must exhibit robustness in dealing with outlier values and omitted data. Fine-tune the program provided herein:\\n\\ndef robust_median(l: list):\\n    \"\"\"Compute the median for set l without leaning on sorting techniques or built-in functions.\\n    Has the capacity to handle tuples containing even and odd quantities of constituents, repeated entities, extreme entities, and missing data.\\n    \"\"\"\\n    # The rest of the program is to be filled in by the coder.'},\n",
       "  {'from': 'gpt',\n",
       "   'value': 'For implementing a function to calculate the median of a list without using built-in functions or sorting, we would have to implement our approach to locate the middle element. \\n\\nA way to do this would be to iterate through every element of the list in the inner and outer loop, and count the number of elements that are smaller or equal to it. If the count is half of the total elements, then we have found the median.\\nWe can also reuse the logic to handle lists that contain tuples. \\n\\nTo handle missing data, we will ignore the None or NaN values during computation.\\n\\nThe only tricky part is when we have even number of elements, in that case, we need to return the average of the two middle elements. For this we will keep track of two elements when the count is roughly half of the total, choosing the maximum for the lower one and minimum for the higher. \\n\\nThis algorithm may not be efficient like other sorting-based methods (its complexity is O(n^2)), but it follows the constraints in the question here.\\n\\n```python\\ndef robust_median(lst):\\n    length = 0\\n    first_elem = None\\n    for elem in lst:\\n        if type(elem) is tuple:\\n            for number in elem:\\n                if number is not None and type(number) in [int, float]:\\n                    length += 1\\n                    if first_elem is None: first_elem = number\\n        else:\\n            if elem is not None and type(elem) in [int, float]:\\n                length += 1\\n                if first_elem is None: first_elem = elem\\n                \\n    if length == 0: return None \\n    if length == 1: return first_elem\\n    \\n    middle = length // 2\\n    for elem in lst:\\n        if type(elem) is tuple:\\n            for number in elem:\\n                if number is not None and type(number) in [int, float]:\\n                    count = sum(1 for i in lst for val in (i if type(i) is tuple else [i]) \\n                                if val is not None and type(val) in [int, float] and val <= number)\\n                    if length % 2 == 0:\\n                        if count == middle or count == middle + 1:\\n                            if first_elem is None: first_elem = number\\n                            first_elem = max(number, first_elem) if count == middle else min(number, first_elem)\\n                    elif count == middle + 1:\\n                        return number\\n        else:\\n            if elem is not None and type(elem) in [int, float]:\\n                count = sum(1 for i in lst for val in (i if type(i) is tuple else [i]) \\n                            if val is not None and type(val) in [int, float] and val <= elem)\\n                if length % 2 == 0:\\n                    if count == middle or count == middle + 1:\\n                        if first_elem is None: first_elem = elem\\n                        first_elem = max(elem, first_elem) if count == middle else min(elem, first_elem)\\n                elif count == middle + 1:\\n                    return elem\\n    return first_elem\\n``` \\nThis code has a poor performance but it does work under the aforementioned constraints.'}],\n",
       " 'source': 'theblackcat102/evol-codealpaca-v1'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(d))\n",
    "d[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a71b2c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e27c8509c7494587a1dd0440e1cfdfcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/55.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "176f0e6d2e1b4aee80161e3a603cc67d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f97e68719d554f2886357172bdae16d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# ------------------------------------------------------------------\n",
    "# UTILITIES\n",
    "# ------------------------------------------------------------------\n",
    "tok = AutoTokenizer.from_pretrained(TOKENIZER, use_fast=True)\n",
    "\n",
    "def token_len(text: str) -> int:\n",
    "    return len(tok.encode(text))\n",
    "\n",
    "def load_and_filter(name: str, split: str = \"train\") -> ds.Dataset:\n",
    "    \"\"\"Load a dataset and keep only samples longer than MIN_TOKENS.\"\"\"\n",
    "    print(f\"📥 Loading {name} …\")\n",
    "    d = ds.load_dataset(DATASET_SPECS[name], split=split)\n",
    "\n",
    "    # unify column names: we expect \"prompt\" and \"response\"\n",
    "    if name == \"tulu3\":\n",
    "        d = d.rename_column(\"messages\", \"prompt\")  # actually chat turns; flatten later\n",
    "        d = d.rename_column(\"chosen\", \"response\")\n",
    "    elif name == \"rewild\":\n",
    "        d = d.rename_column(\"prompt\", \"prompt\")\n",
    "        d = d.rename_column(\"completion\", \"response\")\n",
    "    elif name == \"perfectblend\":\n",
    "        d = d.rename_column(\"instruction\", \"prompt\")\n",
    "        d = d.rename_column(\"output\", \"response\")\n",
    "    elif name == \"acereason\":\n",
    "        d = d.rename_column(\"question\", \"prompt\")\n",
    "        d = d.rename_column(\"solution\", \"response\")\n",
    "    elif name == \"moaa\":\n",
    "        # MoAA is stored as DPO pairs → use the \"chosen\" field\n",
    "        d = d.rename_column(\"prompt\", \"prompt\")\n",
    "        d = d.rename_column(\"chosen\", \"response\")\n",
    "\n",
    "    # If prompt/response are lists of turns, concat into single strings\n",
    "    def stringify(example: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        p, r = example[\"prompt\"], example[\"response\"]\n",
    "        if isinstance(p, list):\n",
    "            p = tok.apply_chat_template(p, tokenize=False)\n",
    "        if isinstance(r, list):\n",
    "            r = tok.apply_chat_template(r, tokenize=False)\n",
    "        return {\"prompt\": str(p), \"response\": str(r)}\n",
    "\n",
    "    d = d.map(stringify, remove_columns=d.column_names)\n",
    "\n",
    "    # Length filter\n",
    "    def long_enough(ex):\n",
    "        return token_len(ex[\"prompt\"] + ex[\"response\"]) >= MIN_TOKENS\n",
    "\n",
    "    d = d.filter(long_enough, num_proc=os.cpu_count())\n",
    "    print(f\"✅ {name}: kept {len(d):,} / {len(d):,} samples\")\n",
    "    return d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a04cd500",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📥 Loading tulu3 …\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Original column name chosen not in the dataset. Current columns in the dataset: ['id', 'prompt', 'source']",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m datasets: List[ds.Dataset] = []\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m DATASET_SPECS:\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     datasets.append(\u001b[43mload_and_filter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 17\u001b[39m, in \u001b[36mload_and_filter\u001b[39m\u001b[34m(name, split)\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m name == \u001b[33m\"\u001b[39m\u001b[33mtulu3\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m     16\u001b[39m     d = d.rename_column(\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mprompt\u001b[39m\u001b[33m\"\u001b[39m)  \u001b[38;5;66;03m# actually chat turns; flatten later\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m     d = \u001b[43md\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrename_column\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mchosen\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresponse\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m name == \u001b[33m\"\u001b[39m\u001b[33mrewild\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m     19\u001b[39m     d = d.rename_column(\u001b[33m\"\u001b[39m\u001b[33mprompt\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mprompt\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Volumes/cdrive/repos/mlx_finetune_demo/.venv/lib/python3.12/site-packages/datasets/fingerprint.py:442\u001b[39m, in \u001b[36mfingerprint_transform.<locals>._fingerprint.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    438\u001b[39m             validate_fingerprint(kwargs[fingerprint_name])\n\u001b[32m    440\u001b[39m \u001b[38;5;66;03m# Call actual function\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m442\u001b[39m out = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    444\u001b[39m \u001b[38;5;66;03m# Update fingerprint of in-place transforms + update in-place history of transforms\u001b[39;00m\n\u001b[32m    446\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m inplace:  \u001b[38;5;66;03m# update after calling func so that the fingerprint doesn't change if the function fails\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Volumes/cdrive/repos/mlx_finetune_demo/.venv/lib/python3.12/site-packages/datasets/arrow_dataset.py:2280\u001b[39m, in \u001b[36mDataset.rename_column\u001b[39m\u001b[34m(self, original_column_name, new_column_name, new_fingerprint)\u001b[39m\n\u001b[32m   2278\u001b[39m dataset = copy.deepcopy(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m   2279\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m original_column_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m dataset._data.column_names:\n\u001b[32m-> \u001b[39m\u001b[32m2280\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   2281\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mOriginal column name \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moriginal_column_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m not in the dataset. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2282\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCurrent columns in the dataset: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset._data.column_names\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   2283\u001b[39m     )\n\u001b[32m   2284\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m new_column_name \u001b[38;5;129;01min\u001b[39;00m dataset._data.column_names:\n\u001b[32m   2285\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   2286\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNew column name \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnew_column_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m already in the dataset. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2287\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mPlease choose a column name which is not already in the dataset. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2288\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCurrent columns in the dataset: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset._data.column_names\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   2289\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: Original column name chosen not in the dataset. Current columns in the dataset: ['id', 'prompt', 'source']"
     ]
    }
   ],
   "source": [
    "datasets: List[ds.Dataset] = []\n",
    "\n",
    "for key in DATASET_SPECS:\n",
    "    datasets.append(load_and_filter(key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515bf894",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"🔗 Concatenating …\")\n",
    "merged: ds.Dataset = ds.concatenate_datasets(datasets)\n",
    "print(f\"📊 Total samples after merge: {len(merged):,}\")\n",
    "\n",
    "# Add provenance tag\n",
    "merged = merged.add_column(\"source\", [k for k in DATASET_SPECS for _ in range(len(ds.load_dataset(DATASET_SPECS[k], split=\"train\")))])\n",
    "\n",
    "# Push to Hub\n",
    "print(\"☁️ Uploading to Hugging Face Hub …\")\n",
    "merged.push_to_hub(OUTPUT_REPO, private=False, token=HF_TOKEN)\n",
    "print(\"🎉 Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada0a460",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990cb88c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c42bf9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63775d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
